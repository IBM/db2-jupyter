{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Db2 Jupyter Notebook Magic Commands Jupyter notebooks include the ability to extend the syntax available within code blocks with a feature called Magic commands. Magic commands start with a percent sign % and provide a variety of features within a notebook environment, including modifying notebook behavior, executing OS commands, extending notebook functionality and with Db2 magic, a way of interacting with a Db2 database. Once you have loaded the Db2 magic commands into your notebook, you are able to query Db2 tables using standard SQL syntax: Db2 magic commands provide a number of features that will simplify your use of Db2 databases, including: Simplified connections to data sources Ability to store result sets into Pandas dataframes Create Db2 tables from Pandas dataframes and populate the tables with the data frame contents Run the majority of DCL, DDL, and DML statements that are available in Db2 Create functions, stored procedures, and run a subset of administrative commands Allow for parallel execution of SQL queries even on non-warehousing systems And much more! The following sections will describe how to get started with the Db2 magic commands as well as the prerequisites that are required to run the code in your environment.","title":"Home"},{"location":"#db2-jupyter-notebook-magic-commands","text":"Jupyter notebooks include the ability to extend the syntax available within code blocks with a feature called Magic commands. Magic commands start with a percent sign % and provide a variety of features within a notebook environment, including modifying notebook behavior, executing OS commands, extending notebook functionality and with Db2 magic, a way of interacting with a Db2 database. Once you have loaded the Db2 magic commands into your notebook, you are able to query Db2 tables using standard SQL syntax: Db2 magic commands provide a number of features that will simplify your use of Db2 databases, including: Simplified connections to data sources Ability to store result sets into Pandas dataframes Create Db2 tables from Pandas dataframes and populate the tables with the data frame contents Run the majority of DCL, DDL, and DML statements that are available in Db2 Create functions, stored procedures, and run a subset of administrative commands Allow for parallel execution of SQL queries even on non-warehousing systems And much more! The following sections will describe how to get started with the Db2 magic commands as well as the prerequisites that are required to run the code in your environment.","title":"Db2 Jupyter Notebook Magic Commands"},{"location":"command_options/","text":"Db2 Magic Options The previous section discussed options that are specific for %sql commands and are only valid during the execution of that statement. There are options available that impact the execution of the %sql statements and are discussed below. There are four options that can be set with the %sql command. These options are shown below with the default value shown in parentheses. DISPLAY PANDAS | GRID (PANDAS) Display the results as a PANDAS dataframe (default) or as a scrollable GRID. MAXROWS n (10) The maximum number of rows that will be displayed before summary information is shown. If the answer set is less than this number of rows, it will be completely shown on the screen. If the answer set is larger than this amount, only the first 5 rows and last 5 rows of the answer set will be displayed. If you want to display a very large answer set, you may want to consider using the grid option -grid to display the results in a scrollable table. If you really want to show all results, then setting MAXROWS to -1 will return all output. MAXGRID n (5) The maximum size of a grid display. When displaying a result set in a grid -grid , the default size of the display window is 5 rows. You can set this to a larger size so that more rows are shown on the screen. Note that the minimum size always remains at 5 which means that if the system is unable to display your maximum row size it will reduce the table display until it fits. LIST Display the current settings. To set an option use either of the following commands: %sql SET OPTION value %sql OPTION setting value [setting value...] The OPTION command allows multiple options to be set with one command. When either command is executed, the current settings are displayed. For instance, to set the maximum display rows to 100: %sql SET MAXROWS 100 Display Format The default display format for any data returned is Pandas. This example illustrates the standard output returned from a SELECT statement: The Pandas output format will sometimes truncate the output (as shown above). You can adjust the number of rows that can be displayed from an answer set using the MAXROWS setting discussed below. This standard output may be sufficient for your requirements if you are only returning small answer sets (less than 100 rows to display). Otherwise, you may consider using the -grid option. The same answer set using a -grid display is shown below. The advantage of the -grid option is that the entire answer set can be retrieved and reviewed in the scrollable window. This is not possible when using Pandas output. It is also not possible to display all rows in the Pandas dataframe due to memory limitations when writing to a Jupyter notebook cell. The grid control removes these restrictions by allowing the entire answer set to be viewed in a grid format. The DISPLAY option is used to set the default display format of an SQL answer set. The default setting is PANDAS which is the standard output format of a Pandas dataframe. To change the default display format use the SET command with the DISPLAY option: %sql SET DISPLAY [PANDAS | GRID] Setting this value to GRID will result in all answer sets being displayed using the interactive grid format. %sql SET DISPLAY GRID The display format is kept with the connection information. If you start up another session using the same connection information, the last display format will be used. Maximum Number of Rows The default number of rows that are displayed when using a Pandas dataframe is 10. An answer set will always display the first 5 rows and the last 5 rows of an answer set in a Jupyter notebook: The missing rows are marked with a ... in-between the top and bottom sections of the answer set. To increase the default display size, you can change the MAXROWS setting. The rows that are displayed depends on how many can fit within the MAXROWS settings. The rules are as follows: If the number of rows is less than or equal to MAXROWS , then all rows will be displayed. If the number of rows is greater than MAXROWS then only the top and bottom 5 rows are displayed. If MAXROWS is too big for a Jupyter notebook cell, the cell will contain scroll bars allowing you to scroll through the results. The maximum value for MAXROWS is 100 to avoid memory issues with Jupyter notebooks. For the following examples, the MAXROWS setting is set to 5 . %sql SET MAXROWS 5 The first example demonstrates when exactly 5 rows are returned from the EMPLOYEE table. The next example demonstrates how the answer set is split when the number of rows exceeds 5. If maximum value of MAXROWS is 100. As many rows possible will be displayed in the notebook. If the number of rows is too large, the cell will contain scrollbars so that you can see all rows that are returned. If you want to scroll through large answer sets, you should consider using the -grid flag or set the DISPLAY option. Maximum Grid Size The grid control displays approximately 5 lines of data in a Jupyter notebook cell. The MAXGRID option can be used to change the number of rows that the grid control will display in a Jupyter notebook cell. The entire answer set can be reviewed using the scroll bars on the right side and bottom of the control. The following is the default view of the grid control. To change the number of rows displayed, use the MAXGRID option: %sql SET MAXGRID 10 The answer set will now display a minimum of 10 rows in the grid. Note that the MAXGRID option does not affect the number of rows that are returned in an answer set. Thread Parallelism The Db2 magic commands have the ability to split a SQL statement into multiple threads to improve the time it takes to retrieve answer sets. This feature does not make Db2 more efficient - it provides a way to increase the utilization of Db2 and pipeline SQL statements so that less time is spent waiting for data to be retrieved. The section on parallelism explores this feature in more detail. The THREADS option tells Db2 the maximum of threads that it will be allowed to use when running the SQL. The SQL will be split (based on some range or key) and multithreaded into Db2. The THREADS value can be anything from zero (standard SQL call) to 12. The highest performance was found to occur when using 6 to 8 threads, but this will be entirely dependent on the SQL workload. To set the number of threads use the following syntax: %sql SET THREADS 0-12 The THREADS option does not apply to standard SQL statements. This option will only be used when the USING x SELECT... syntax is detected in a %sql or %%sql block.","title":"Db2 Magic Command Options"},{"location":"command_options/#db2-magic-options","text":"The previous section discussed options that are specific for %sql commands and are only valid during the execution of that statement. There are options available that impact the execution of the %sql statements and are discussed below. There are four options that can be set with the %sql command. These options are shown below with the default value shown in parentheses. DISPLAY PANDAS | GRID (PANDAS) Display the results as a PANDAS dataframe (default) or as a scrollable GRID. MAXROWS n (10) The maximum number of rows that will be displayed before summary information is shown. If the answer set is less than this number of rows, it will be completely shown on the screen. If the answer set is larger than this amount, only the first 5 rows and last 5 rows of the answer set will be displayed. If you want to display a very large answer set, you may want to consider using the grid option -grid to display the results in a scrollable table. If you really want to show all results, then setting MAXROWS to -1 will return all output. MAXGRID n (5) The maximum size of a grid display. When displaying a result set in a grid -grid , the default size of the display window is 5 rows. You can set this to a larger size so that more rows are shown on the screen. Note that the minimum size always remains at 5 which means that if the system is unable to display your maximum row size it will reduce the table display until it fits. LIST Display the current settings. To set an option use either of the following commands: %sql SET OPTION value %sql OPTION setting value [setting value...] The OPTION command allows multiple options to be set with one command. When either command is executed, the current settings are displayed. For instance, to set the maximum display rows to 100: %sql SET MAXROWS 100","title":"Db2 Magic Options"},{"location":"command_options/#display-format","text":"The default display format for any data returned is Pandas. This example illustrates the standard output returned from a SELECT statement: The Pandas output format will sometimes truncate the output (as shown above). You can adjust the number of rows that can be displayed from an answer set using the MAXROWS setting discussed below. This standard output may be sufficient for your requirements if you are only returning small answer sets (less than 100 rows to display). Otherwise, you may consider using the -grid option. The same answer set using a -grid display is shown below. The advantage of the -grid option is that the entire answer set can be retrieved and reviewed in the scrollable window. This is not possible when using Pandas output. It is also not possible to display all rows in the Pandas dataframe due to memory limitations when writing to a Jupyter notebook cell. The grid control removes these restrictions by allowing the entire answer set to be viewed in a grid format. The DISPLAY option is used to set the default display format of an SQL answer set. The default setting is PANDAS which is the standard output format of a Pandas dataframe. To change the default display format use the SET command with the DISPLAY option: %sql SET DISPLAY [PANDAS | GRID] Setting this value to GRID will result in all answer sets being displayed using the interactive grid format. %sql SET DISPLAY GRID The display format is kept with the connection information. If you start up another session using the same connection information, the last display format will be used.","title":"Display Format"},{"location":"command_options/#maximum-number-of-rows","text":"The default number of rows that are displayed when using a Pandas dataframe is 10. An answer set will always display the first 5 rows and the last 5 rows of an answer set in a Jupyter notebook: The missing rows are marked with a ... in-between the top and bottom sections of the answer set. To increase the default display size, you can change the MAXROWS setting. The rows that are displayed depends on how many can fit within the MAXROWS settings. The rules are as follows: If the number of rows is less than or equal to MAXROWS , then all rows will be displayed. If the number of rows is greater than MAXROWS then only the top and bottom 5 rows are displayed. If MAXROWS is too big for a Jupyter notebook cell, the cell will contain scroll bars allowing you to scroll through the results. The maximum value for MAXROWS is 100 to avoid memory issues with Jupyter notebooks. For the following examples, the MAXROWS setting is set to 5 . %sql SET MAXROWS 5 The first example demonstrates when exactly 5 rows are returned from the EMPLOYEE table. The next example demonstrates how the answer set is split when the number of rows exceeds 5. If maximum value of MAXROWS is 100. As many rows possible will be displayed in the notebook. If the number of rows is too large, the cell will contain scrollbars so that you can see all rows that are returned. If you want to scroll through large answer sets, you should consider using the -grid flag or set the DISPLAY option.","title":"Maximum Number of Rows"},{"location":"command_options/#maximum-grid-size","text":"The grid control displays approximately 5 lines of data in a Jupyter notebook cell. The MAXGRID option can be used to change the number of rows that the grid control will display in a Jupyter notebook cell. The entire answer set can be reviewed using the scroll bars on the right side and bottom of the control. The following is the default view of the grid control. To change the number of rows displayed, use the MAXGRID option: %sql SET MAXGRID 10 The answer set will now display a minimum of 10 rows in the grid. Note that the MAXGRID option does not affect the number of rows that are returned in an answer set.","title":"Maximum Grid Size"},{"location":"command_options/#thread-parallelism","text":"The Db2 magic commands have the ability to split a SQL statement into multiple threads to improve the time it takes to retrieve answer sets. This feature does not make Db2 more efficient - it provides a way to increase the utilization of Db2 and pipeline SQL statements so that less time is spent waiting for data to be retrieved. The section on parallelism explores this feature in more detail. The THREADS option tells Db2 the maximum of threads that it will be allowed to use when running the SQL. The SQL will be split (based on some range or key) and multithreaded into Db2. The THREADS value can be anything from zero (standard SQL call) to 12. The highest performance was found to occur when using 6 to 8 threads, but this will be entirely dependent on the SQL workload. To set the number of threads use the following syntax: %sql SET THREADS 0-12 The THREADS option does not apply to standard SQL statements. This option will only be used when the USING x SELECT... syntax is detected in a %sql or %%sql block.","title":"Thread Parallelism"},{"location":"connect/","text":"Connecting to Db2 Before any SQL commands can be issued, a connection needs to be made to the Db2 database that you will be using. The connection can be done manually (through the use of the CONNECT command), or automatically when the first %sql command is issued. The Db2 magic command tracks whether a connection has occurred in the past and saves this information between notebooks and sessions. When you start up a notebook and issue a command, the program will reconnect to the database using your credentials from the last session. In the event that you have not connected before, the system will require that you execute the CONNECT command. To connect to a Db2 database you will need the following information: Database name Hostname Db2 port number Userid Password You may also require two additional pieces of information: SSL Security - Request that SSL connections be used with Db2 and optionally the location of the SSL file that contains required certificate information SSL File - The certificate file (if required) API Key - An API key that is provided to connect to Db2 Access Token - An access token that is provided to connect to Db2 DSN - A string that contains the ODBC connection string The information supplied will be stored in the directory that the notebooks are stored on. Once you have entered the information, the system will attempt to connect to the database for you, and then you can run all SQL scripts. More details on the CONNECT syntax will be found in a section below. If you have credentials available from Db2 on Cloud or CP4D, place the contents of the credentials into a variable and then use the CONNECT CREDENTIALS <var> syntax to connect to the database. db2blu = { \"uid\" : \"xyz123456\", ...} %sql CONNECT CREDENTIALS db2blu If the connection is successful using the credentials, the variable will be saved to disk so that you can connect from within another notebook using the same syntax. Connect Syntax The CONNECT command has four different options that are listed below. You must explicitly connect to a Db2 database for the first time. Subsequent connections are not required if you are connecting to the same database. When the Db2 magic command executes your SQL command, it checks to see whether you have already connected to the database. If not, it will check to see if there was a successful connection previously and will use those credentials to reconnect. You can also force a connection to the previous database by issuing the CONNECT command with no parameters. Standard Connection The format of the CONNECT command is: %sql CONNECT TO database USER userid USING password | ? HOST ip_address PORT port_number [ SSL ] [ SSLFILE file ] [ APIKEY key ] [ ACCESSTOKEN token ] [ DNS \"string\" ] The required fields are: Database - Database name you want to connect to Hostname - localhost if Db2 is running on your own machine or the IP address or symbolic name of the server PORT - The port to use for connecting to Db2. This is usually 50000. When connecting using an apikey or access token, you will need to provide the key value after the keyword. The SSL security settings are already preconfigured when using these settings. APIKEY value - the value of the APIKEY ACCESSTOKEN value - the value of the access token If not using APIKEY/ACCESSTOKEN, you will need to provide your credentials to connect: Userid - The userid to use when connecting to the database. Password - The password for the userid. Use a question mark ? to be prompted to enter the password without displaying the text. There are two optional fields. The SSL option ensures that the transmission of the SQL and answer sets are encrypted \"on the wire\". SSL is typically required when using public networks to communicate to a Db2 database. For internal systems this may not be the case. If you are required to use SSL communication, you must specify SSL in the connection string. The use of an SSL port on Db2 (traditionally 50001) may require a certificate file. If your workstation has already been configured to use SSL communication, then only the SSL parameter is required. If you have been given access to a Db2 database and have been provided with an SSL certificate, then you will use the SSLFILE option. The certificate option requires the location and name of the file that contains the SSL certificate which is used for communicating with Db2. Normally this certificate would have been provided to you and placed onto your local file system. The format of the SSLFILE certificate option is SSLFILE filename where filename is the fully qualified location of the SSL file. You do not need to specify SSL as part of the connection string since it is assumed that you will be using SSL communication when you supply a certificate file. %sql CONNECT TO BLUDB USER BLUADMIN USING ? HOST 145.23.24.1 PORT 50001 SSLFILE /ssl/bludb.cert If you want to use an ODBC connection string, rather than using the CONNECT syntax, you can supply it by using the DSN option. DSN \"dsn string\" The value in the \"dsn string\" will be passed to the connect routine without modification. A sample DSN string is found below: dsn = \"DRIVER={IBM DB2 ODBC DRIVER};DATABASE=BLUDB;HOSTNAME=some.random.location.com;PORT=12345;PROTOCOL=TCPIP;ConnectTimeout=15;LONGDATACOMPAT=1;AUTHENTICATION=GSSplugin;SECURITY=SSL;APIKEY=lKJH896HUGY102938unad9eYtw;\" %sql CONNECT DSN \"{dsn}\" Note how the DSN string must be surrounded in quotes in the CONNECT command. Details of all Db2 CLI settings can be found in the Db2 ODBC/CLI documentation . Passwords If you use a ? for the password field, the system will prompt you for a password. This avoids typing the password as clear text on the screen. If a connection is not successful, the system will print the error message associated with the connect request. Connection with Credentials The CREDENTIALS option allows you to use credentials that are supplied by Db2 on Cloud instances. The credentials can be supplied as a variable and if successful, the variable will be saved to disk for future use. If you create another notebook and use the identical syntax, if the variable is not defined, the contents on disk will be used as the credentials. You should assign the credentials to a variable that represents the database (or schema) that you are communicating with. Using familiar names makes it easier to remember the credentials when connecting. Credentials must be supplied as a JSON dictionary that contains the following information: { \"database\" : \"bludb\", \"hostname\" : \"somesite-txn-blue.net\", \"port\" : 50000, \"pwd\" : \"somethingrandom\", \"uid\" : \"myuserid\", \"ssl\" : \"SSL\", \"sslfile\" : \"filename\", \"apikey\" : \"apikey\", \"accesstoken\" : \"accesstoken\", \"dsn\" : \"dsn\" } Some of these fields are not necessary. For instance, the apikey, accesstoken, and dsn fields are only required if you are using these forms of connections. To use this as a connection document, assign this dictionary value to a variable: bludb = { \"hostname\": \"somesite-txn-blue.net\", \"pwd\": \"somethingrandom\",... } Then use the following syntax to connect to the database: %sql CONNECT CREDENTIALS bludb The system will attempt to connect to the database using the following process: If the bludb variable exists in your Jupyter notebook, it will use the contents of that variable to connect to the database. If bludb does not exist, the program will check the local directory that the notebook is in to find a file called bludb.pickle which contains the credentials. If neither of the above exist the program will issue a connection error. If a connection was successfully made, the credentials are written to disk using the name of the variable as the file name and the file type being pickle . A pickle file is a Python format that allows structures and data types to be written to a file and easily read back in. Note that if you want to use the traditional CONNECT syntax, you can extract the values required from the credentials file. You will need the following: db - Database name uid - Connection userid pwd - Connection password hostname - Host port - Port number SSL - Add this keyword if you need to connect via SSL Closing a Connection CONNECT CLOSE will close the current connection, but will not reset the database parameters. This means that if you issue the CONNECT command again, the system will reconnect you to the database. %sql CONNECT CLOSE If you issue another SQL statement after closing the connection, the program will reconnect to last database you were connected to. If you don't want this behavior, use the CONNECT RESET command. Reseting a Connection CONNECT RESET will close the current connection and remove any information on the connection. You will need to issue a new CONNECT statement with the connection information. %sql CONNECT RESET Issuing a SQL statement after a CONNECT RESET will result in a connection error.","title":"Getting Connected"},{"location":"connect/#connecting-to-db2","text":"Before any SQL commands can be issued, a connection needs to be made to the Db2 database that you will be using. The connection can be done manually (through the use of the CONNECT command), or automatically when the first %sql command is issued. The Db2 magic command tracks whether a connection has occurred in the past and saves this information between notebooks and sessions. When you start up a notebook and issue a command, the program will reconnect to the database using your credentials from the last session. In the event that you have not connected before, the system will require that you execute the CONNECT command. To connect to a Db2 database you will need the following information: Database name Hostname Db2 port number Userid Password You may also require two additional pieces of information: SSL Security - Request that SSL connections be used with Db2 and optionally the location of the SSL file that contains required certificate information SSL File - The certificate file (if required) API Key - An API key that is provided to connect to Db2 Access Token - An access token that is provided to connect to Db2 DSN - A string that contains the ODBC connection string The information supplied will be stored in the directory that the notebooks are stored on. Once you have entered the information, the system will attempt to connect to the database for you, and then you can run all SQL scripts. More details on the CONNECT syntax will be found in a section below. If you have credentials available from Db2 on Cloud or CP4D, place the contents of the credentials into a variable and then use the CONNECT CREDENTIALS <var> syntax to connect to the database. db2blu = { \"uid\" : \"xyz123456\", ...} %sql CONNECT CREDENTIALS db2blu If the connection is successful using the credentials, the variable will be saved to disk so that you can connect from within another notebook using the same syntax.","title":"Connecting to Db2"},{"location":"connect/#connect-syntax","text":"The CONNECT command has four different options that are listed below. You must explicitly connect to a Db2 database for the first time. Subsequent connections are not required if you are connecting to the same database. When the Db2 magic command executes your SQL command, it checks to see whether you have already connected to the database. If not, it will check to see if there was a successful connection previously and will use those credentials to reconnect. You can also force a connection to the previous database by issuing the CONNECT command with no parameters.","title":"Connect Syntax"},{"location":"connect/#standard-connection","text":"The format of the CONNECT command is: %sql CONNECT TO database USER userid USING password | ? HOST ip_address PORT port_number [ SSL ] [ SSLFILE file ] [ APIKEY key ] [ ACCESSTOKEN token ] [ DNS \"string\" ] The required fields are: Database - Database name you want to connect to Hostname - localhost if Db2 is running on your own machine or the IP address or symbolic name of the server PORT - The port to use for connecting to Db2. This is usually 50000. When connecting using an apikey or access token, you will need to provide the key value after the keyword. The SSL security settings are already preconfigured when using these settings. APIKEY value - the value of the APIKEY ACCESSTOKEN value - the value of the access token If not using APIKEY/ACCESSTOKEN, you will need to provide your credentials to connect: Userid - The userid to use when connecting to the database. Password - The password for the userid. Use a question mark ? to be prompted to enter the password without displaying the text. There are two optional fields. The SSL option ensures that the transmission of the SQL and answer sets are encrypted \"on the wire\". SSL is typically required when using public networks to communicate to a Db2 database. For internal systems this may not be the case. If you are required to use SSL communication, you must specify SSL in the connection string. The use of an SSL port on Db2 (traditionally 50001) may require a certificate file. If your workstation has already been configured to use SSL communication, then only the SSL parameter is required. If you have been given access to a Db2 database and have been provided with an SSL certificate, then you will use the SSLFILE option. The certificate option requires the location and name of the file that contains the SSL certificate which is used for communicating with Db2. Normally this certificate would have been provided to you and placed onto your local file system. The format of the SSLFILE certificate option is SSLFILE filename where filename is the fully qualified location of the SSL file. You do not need to specify SSL as part of the connection string since it is assumed that you will be using SSL communication when you supply a certificate file. %sql CONNECT TO BLUDB USER BLUADMIN USING ? HOST 145.23.24.1 PORT 50001 SSLFILE /ssl/bludb.cert If you want to use an ODBC connection string, rather than using the CONNECT syntax, you can supply it by using the DSN option. DSN \"dsn string\" The value in the \"dsn string\" will be passed to the connect routine without modification. A sample DSN string is found below: dsn = \"DRIVER={IBM DB2 ODBC DRIVER};DATABASE=BLUDB;HOSTNAME=some.random.location.com;PORT=12345;PROTOCOL=TCPIP;ConnectTimeout=15;LONGDATACOMPAT=1;AUTHENTICATION=GSSplugin;SECURITY=SSL;APIKEY=lKJH896HUGY102938unad9eYtw;\" %sql CONNECT DSN \"{dsn}\" Note how the DSN string must be surrounded in quotes in the CONNECT command. Details of all Db2 CLI settings can be found in the Db2 ODBC/CLI documentation .","title":"Standard Connection"},{"location":"connect/#passwords","text":"If you use a ? for the password field, the system will prompt you for a password. This avoids typing the password as clear text on the screen. If a connection is not successful, the system will print the error message associated with the connect request.","title":"Passwords"},{"location":"connect/#connection-with-credentials","text":"The CREDENTIALS option allows you to use credentials that are supplied by Db2 on Cloud instances. The credentials can be supplied as a variable and if successful, the variable will be saved to disk for future use. If you create another notebook and use the identical syntax, if the variable is not defined, the contents on disk will be used as the credentials. You should assign the credentials to a variable that represents the database (or schema) that you are communicating with. Using familiar names makes it easier to remember the credentials when connecting. Credentials must be supplied as a JSON dictionary that contains the following information: { \"database\" : \"bludb\", \"hostname\" : \"somesite-txn-blue.net\", \"port\" : 50000, \"pwd\" : \"somethingrandom\", \"uid\" : \"myuserid\", \"ssl\" : \"SSL\", \"sslfile\" : \"filename\", \"apikey\" : \"apikey\", \"accesstoken\" : \"accesstoken\", \"dsn\" : \"dsn\" } Some of these fields are not necessary. For instance, the apikey, accesstoken, and dsn fields are only required if you are using these forms of connections. To use this as a connection document, assign this dictionary value to a variable: bludb = { \"hostname\": \"somesite-txn-blue.net\", \"pwd\": \"somethingrandom\",... } Then use the following syntax to connect to the database: %sql CONNECT CREDENTIALS bludb The system will attempt to connect to the database using the following process: If the bludb variable exists in your Jupyter notebook, it will use the contents of that variable to connect to the database. If bludb does not exist, the program will check the local directory that the notebook is in to find a file called bludb.pickle which contains the credentials. If neither of the above exist the program will issue a connection error. If a connection was successfully made, the credentials are written to disk using the name of the variable as the file name and the file type being pickle . A pickle file is a Python format that allows structures and data types to be written to a file and easily read back in. Note that if you want to use the traditional CONNECT syntax, you can extract the values required from the credentials file. You will need the following: db - Database name uid - Connection userid pwd - Connection password hostname - Host port - Port number SSL - Add this keyword if you need to connect via SSL","title":"Connection with Credentials"},{"location":"connect/#closing-a-connection","text":"CONNECT CLOSE will close the current connection, but will not reset the database parameters. This means that if you issue the CONNECT command again, the system will reconnect you to the database. %sql CONNECT CLOSE If you issue another SQL statement after closing the connection, the program will reconnect to last database you were connected to. If you don't want this behavior, use the CONNECT RESET command.","title":"Closing a Connection"},{"location":"connect/#reseting-a-connection","text":"CONNECT RESET will close the current connection and remove any information on the connection. You will need to issue a new CONNECT statement with the connection information. %sql CONNECT RESET Issuing a SQL statement after a CONNECT RESET will result in a connection error.","title":"Reseting a Connection"},{"location":"developmentsql/","text":"Development SQL The %sql and %%sql commands deals with SQL statements and commands that are run in an interactive manner. There is a class of SQL commands that are more suited to a development environment where code is iterated or requires changing input. The commands that are associated with this form of SQL are: AUTOCOMMIT COMMIT / ROLLBACK PREPARE EXECUTE In addition, the sqlcode , sqlstate and sqlerror fields are populated after every statement so you can use these variables to test for errors. Autocommit is the default manner in which SQL statements are executed. At the end of the successful completion of a statement, the results are committed to the database. There is no concept of a transaction where multiple DML/DDL statements are considered one transaction. The AUTOCOMMIT command allows you to turn autocommit OFF or ON . This means that the set of SQL commands run after the AUTOCOMMIT OFF command are executed are not committed to the database until a COMMIT or ROLLBACK command is issued. COMMIT (WORK) will finalize all transactions (COMMIT) to the database and ROLLBACK will undo the changes. If you issue a SELECT statement during the execution of your block, the results will reflect all of your changes. If you ROLLBACK the transaction, the changes will be lost. PREPARE is typically used in a situation where you want to repeatedly execute an SQL statement with different variables without incurring the SQL compilation overhead. For instance: x = %sql PREPARE SELECT LASTNAME FROM EMPLOYEE WHERE EMPNO=? for y in ['000010','000020','000030']: %sql execute :x using :y EXECUTE is used to execute a previously compiled statement. Autocommit and Commit Scope By default, any SQL statements executed with the %sql magic command are immediately committed. This means that the log file has the transaction details, and the results are committed to disk. In other words, you can't change your mind after the statement finishes execution. This behavior is often referred to as AUTOCOMMIT and adds a level of overhead to statement execution because at the end of every statement the results must be \"hardened\". On the other hand, autocommit means you don't have to worry about explicitly committing work or causing potential locking issues because you are holding up resources. When a record is updated, no other user will be able to view it (unless using \"dirty read\") until you commit. Holding the resource in a lock means that other workloads may come to a halt while they wait for you to commit your work. Here is a classic example of wanting a commit scope that is based on a series of statements: withdrawal = 100 %sql update checking set balance = balance - withdrawal %sql update savings set balance = balance + withdrawal If autocommit is ON, you could have a problem with the transaction if the system failed after the first update statement. You would have taken money out of the checking account, but have not updated the savings account. To make sure that this transaction is run successfully: %sql autocommit off withdrawal = 100 %sql update checking set balance = balance - withdrawal %sql update savings set balance = balance + withdrawal %sql commit work If the transaction fails before the COMMIT WORK , all changes to the database will be rolled back to its original state, thus protecting the integrity of the two tables. AUTOCOMMIT Autocommit can be turned on or off using the following syntax: %sql AUTOCOMMIT ON | OFF If you turn AUTOCOMMIT OFF then you need to make sure that you COMMIT work at the end of your code. If you don't, there is a possibility that you will lose your work if the connection to Db2 is lost. COMMIT, ROLLBACK To COMMIT all changes to the database you must use the following syntax: %sql COMMIT [WORK | HOLD] The command COMMIT or COMMIT WORK are identical and will commit all work to the database. Issuing a COMMIT command also closes all open cursors or statements that are open. If you had created a prepared statement (see section below) then the compiled statement will be no longer valid. By issuing a COMMIT you are releasing the resources and locks that your application may be holding. COMMIT HOLD will allow you to commit your work to disk, but keeps the resources open for further execution. This is useful for situations where you are inserting or updating 1000s of records and do not want to tie up log space waiting for a commit to occur. The following pseudocode gives you an example how this would be used: %sql autocommit off for i = 1 to 1000 %sql insert into x values i if (i / 100 == 0) print i \"Records inserted\" %sql commit work end if end for %sql commit work %sql autocommit on You should always remember to turn AUTOCOMMIT ON at the end of any code block, or you will have to issue COMMIT at the end of any SQL command to commit it to the database. PREPARE and EXECUTE The PREPARE and EXECUTE commands are useful in situations where you want to repeat an SQL statement multiple times while just changing the parameter values. There isn't any benefit from using these statements for simple tasks that may only run occasionally. The benefit of PREPARE and EXECUTE is more evident when dealing with several transactions that are the same. The PREPARE statement can be used against many types of SQL, but in this implementation, only the following SQL statements are supported: SELECT INSERT UPDATE DELETE MERGE To prepare a statement, you must use the following syntax: stmt = %sql PREPARE sql .... The PREPARE statement always returns a statement handle. You must assign the results of the PREPARE statement to a variable since it will be required when you EXECUTE the statement. The SQL statement must have any variables replaced with a question mark ? . For instance, if you wanted to insert a single value into a table you would use the following syntax: stmt = %sql PREPARE insert into x values (?) One important note with parameter markers. If you require the parameter to have a specific data type (say INTEGER) then you may want to place a CAST statement around it to force the proper conversion. Usually strings, integers, decimals, etc., convert fine when using this syntax, but occasionally you may run across a data type issue. For the previous example we could modify it to: stmt = %sql PREPARE insert into x values (CAST(? AS INTEGER)) Once you have prepared a statement, you can execute it using the following syntax: %sql EXECUTE :stmt USING :v1,:v2,:v3,.... You must provide the statement variable :stmt to the EXECUTE statement so that it knows which prepared code to execute. You can create many prepared statements and use them throughout your code. The values that following the USING clause are either constants or SQL host variable names separated by commas. If you place a colon : in front of a variable name, it will be immediately substituted into the statement: %sql EXECUTE :stmt USING 3,'asdsa',24.5,:x,x Variables without a colon in front of them are linked in dynamically. When the EXECUTE statement is processed, the value in the variable is taken directly from memory so that there is no conflict with data type, quotes, or anything that might be interpreted incorrectly. In the sample above we have the same variable specified twice: :x,x Assuming that x has a value of \"Hello\", when the statement is compiled, it will look like: EXECUTE 0x7f2ef37782d0 USING 3,'asdsa',24.5,'Hello',x The contents of :stmt and :x are placed into the string and then the statement is executed. The variable x is left as is. When execution of the prepared statement occurs, the value of the variable x is linked directly to the SQL statement so that the data is retrieved directly from the variable rather than it being materialized in the SQL statement. You can use either format when creating your statements. There is a performance penalty when using the :var format since the value must be materialized into the SQL statement, and then parsed again. When using linked variables you can specify what the underlying data type is so that Db2 does not try to incorrectly translate a value. The previous section mentioned the use of the CAST function to ensure the proper data type is used. With linked variables, you can specify four types of data: char - character data type (default) bin , binary - binary data dec , decimal - decimal data type int , integer - numeric data type These modifiers are added after the variable name by using the @ symbol: %sql EXECUTE :stmt USING v1@int, v2@binary, v3@decimal The default is to treat variables as character strings. Using Arrays and Multiple Parameters When using the PREPARE statement, it can become cumbersome when dealing with many parameter markers. For instance, in order to insert 10 columns into a table the code would look similar to this: stmt = %sql PREPARE INSERT INTO X VALUES (?,?,?,?,?,?,?,?,?,?) The %sql command allows you to use the short form ?*# where # is an integer representing the number of columns you want in the list. The above statement could be written as: stmt = %sql PREPARE INSERT INTO X VALUES (?*10) The syntax can also be used to create groups of parameter markers: stmt = %sql PREPARE INSERT INTO X VALUES (?*3,?*7) While this may seem a strange way of providing parameters, this becomes more useful when we use the EXECUTE command. The EXECUTE command can use Python arrays (lists) as input arguments. For the previous example with 10 parameters you could issue the following command: %sql EXECUTE :stmt USING :v1,:v2,:v3,:v4,:v5,:v6,:v7,:v8,:v9,:v10 If you placed all of these values into an array, you could also do the following: %sql EXECUTE :stmt USING :v[0],:v[1],:v[2],:v[3],:v[4],:v[5],:v[6],:v[7],:v[8],:v[9] That isn't much simpler but shows that you could use items within an array (one dimensional only). The easiest syntax is: %sql EXECUTE :stmt USING :v The only requirement is that the array v has exactly the number of values required to satisfy the parameter list required by the prepared statement. When you split the argument list into groups, you can use multiple arrays to contain the data. Given the following prepare statement: stmt = %sql PREPARE INSERT INTO X VALUES (?*3,?*7) You could execute the statement using two arrays: %sql EXECUTE :stmt USING :name, :details This would work as long as the total number of parameters supplied by the name array and details array is equal to 10. Performance Comparisons The following examples will show the use of AUTOCOMMIT and PREPARE / EXECUTE when running SQL statements. This first SQL statement will load the EMPLOYEE and DEPARTMENT tables (if they don't already exist) and then return an array of the employees in the company using a SELECT statement. The data is returned as a Python list (array) with the -r option. %sql SAMPLEDATA employees = %sql -r select * from employee The employees variable contains the employee data as a Python array. The next statement will retrieve the contents of the first row only (Remember that row 0 contains the name of the columns). We now will create another EMPLOYEE table that is an exact duplicate of what we already have. %%sql -q DROP TABLE EMPLOYEE2; CREATE TABLE EMPLOYEE2 AS (SELECT * FROM EMPLOYEE) DEFINITION ONLY; Loop with INSERT Statements One could always use SQL to insert into this table, but we will use a loop to execute insert statements. The loop will be timed so that we can get a sense of the cost of running this code. In order to make the loop run bit a longer the insert block is run 50 times. Loop with PREPARE Statement An alternative method would be to use a prepared statement that allows us to compile the statement once in Db2 and then reuse the statement in Db2's memory. This method uses the individual column values as input into the EXECUTE statement. Note that this code is run with AUTOCOMMIT OFF which tells Db2 to commit every INSERT statement. Using AUTOCOMMIT OFF results in a lot of additional overhead when running this code. Loop with PREPARE Statement and Arrays You will notice that it is a bit tedious to write out the columns that are required as part of an INSERT statement. A simpler option is to use a Python list or array to and assign it directly in the EXECUTE statement. So rather than: %sql execute :prep using :empno, :firstnme, ... We can just use the array variable generated as part of the for loop: %sql execute :prep using :record The following SQL demonstrates this approach. Loop with PREPARE Statement, Arrays and AUTOCOMMIT OFF Finally, we can turn AUTOCOMMIT off and then commit the work at the end of the block to improve the total time required to insert the data. Note the use of the parameter short form ?*14 in the code. Performance Comparison You may have noticed that the performance of the last method is substantially faster than the other examples. The primary reason for this is the COMMIT only occurring at the end of the code. Run Elapsed Time Insert 3.55 Prepare 2.21 Array 1.47 Commit .67 Error Handling When using PREPARE and EXECUTE statements, the feedback from SQL statements is done via four pre-defined variables: sqlcode - The SQLCODE returned by the last statement executed sqlstate - The SQLSTATE returned by the last statement executed sqlelapsed - The time taken to execute the last statement sqlerror - The error message associated with the sqlcode value When issuing an SQL statement via the %sql command, the feedback is immediate on an error: However, the PREPARE statement does not provide any messages or feedback: This is due to the way PREPARE works. It checks the validity of the SQL, but it doesn't generate an error message because there is a possibility that the object EMPLOYEE4 may exist when the statement gets executed. When you do execute the prepared statement, an error message will be produced. For this reason, the sqlcode should be checked immediately after any EXECUTE statement to make sure that the SQL executed successfully. Usually an sqlcode that is negative is an error that needs to be reviewed. Positive sqlcode value may need to be reviewed, but the following values can usually be ignored: 0 - Statement executed successfully 100 - No records were found Another variable called sqlelapsed may be useful in determining the performance of your SQL statements. The sqlelapsed variable contains the execution time of the SQL, not including the time taken to retrieve the answer set.","title":"Development SQL"},{"location":"developmentsql/#development-sql","text":"The %sql and %%sql commands deals with SQL statements and commands that are run in an interactive manner. There is a class of SQL commands that are more suited to a development environment where code is iterated or requires changing input. The commands that are associated with this form of SQL are: AUTOCOMMIT COMMIT / ROLLBACK PREPARE EXECUTE In addition, the sqlcode , sqlstate and sqlerror fields are populated after every statement so you can use these variables to test for errors. Autocommit is the default manner in which SQL statements are executed. At the end of the successful completion of a statement, the results are committed to the database. There is no concept of a transaction where multiple DML/DDL statements are considered one transaction. The AUTOCOMMIT command allows you to turn autocommit OFF or ON . This means that the set of SQL commands run after the AUTOCOMMIT OFF command are executed are not committed to the database until a COMMIT or ROLLBACK command is issued. COMMIT (WORK) will finalize all transactions (COMMIT) to the database and ROLLBACK will undo the changes. If you issue a SELECT statement during the execution of your block, the results will reflect all of your changes. If you ROLLBACK the transaction, the changes will be lost. PREPARE is typically used in a situation where you want to repeatedly execute an SQL statement with different variables without incurring the SQL compilation overhead. For instance: x = %sql PREPARE SELECT LASTNAME FROM EMPLOYEE WHERE EMPNO=? for y in ['000010','000020','000030']: %sql execute :x using :y EXECUTE is used to execute a previously compiled statement.","title":"Development SQL"},{"location":"developmentsql/#autocommit-and-commit-scope","text":"By default, any SQL statements executed with the %sql magic command are immediately committed. This means that the log file has the transaction details, and the results are committed to disk. In other words, you can't change your mind after the statement finishes execution. This behavior is often referred to as AUTOCOMMIT and adds a level of overhead to statement execution because at the end of every statement the results must be \"hardened\". On the other hand, autocommit means you don't have to worry about explicitly committing work or causing potential locking issues because you are holding up resources. When a record is updated, no other user will be able to view it (unless using \"dirty read\") until you commit. Holding the resource in a lock means that other workloads may come to a halt while they wait for you to commit your work. Here is a classic example of wanting a commit scope that is based on a series of statements: withdrawal = 100 %sql update checking set balance = balance - withdrawal %sql update savings set balance = balance + withdrawal If autocommit is ON, you could have a problem with the transaction if the system failed after the first update statement. You would have taken money out of the checking account, but have not updated the savings account. To make sure that this transaction is run successfully: %sql autocommit off withdrawal = 100 %sql update checking set balance = balance - withdrawal %sql update savings set balance = balance + withdrawal %sql commit work If the transaction fails before the COMMIT WORK , all changes to the database will be rolled back to its original state, thus protecting the integrity of the two tables.","title":"Autocommit and Commit Scope"},{"location":"developmentsql/#autocommit","text":"Autocommit can be turned on or off using the following syntax: %sql AUTOCOMMIT ON | OFF If you turn AUTOCOMMIT OFF then you need to make sure that you COMMIT work at the end of your code. If you don't, there is a possibility that you will lose your work if the connection to Db2 is lost.","title":"AUTOCOMMIT"},{"location":"developmentsql/#commit-rollback","text":"To COMMIT all changes to the database you must use the following syntax: %sql COMMIT [WORK | HOLD] The command COMMIT or COMMIT WORK are identical and will commit all work to the database. Issuing a COMMIT command also closes all open cursors or statements that are open. If you had created a prepared statement (see section below) then the compiled statement will be no longer valid. By issuing a COMMIT you are releasing the resources and locks that your application may be holding. COMMIT HOLD will allow you to commit your work to disk, but keeps the resources open for further execution. This is useful for situations where you are inserting or updating 1000s of records and do not want to tie up log space waiting for a commit to occur. The following pseudocode gives you an example how this would be used: %sql autocommit off for i = 1 to 1000 %sql insert into x values i if (i / 100 == 0) print i \"Records inserted\" %sql commit work end if end for %sql commit work %sql autocommit on You should always remember to turn AUTOCOMMIT ON at the end of any code block, or you will have to issue COMMIT at the end of any SQL command to commit it to the database.","title":"COMMIT, ROLLBACK"},{"location":"developmentsql/#prepare-and-execute","text":"The PREPARE and EXECUTE commands are useful in situations where you want to repeat an SQL statement multiple times while just changing the parameter values. There isn't any benefit from using these statements for simple tasks that may only run occasionally. The benefit of PREPARE and EXECUTE is more evident when dealing with several transactions that are the same. The PREPARE statement can be used against many types of SQL, but in this implementation, only the following SQL statements are supported: SELECT INSERT UPDATE DELETE MERGE To prepare a statement, you must use the following syntax: stmt = %sql PREPARE sql .... The PREPARE statement always returns a statement handle. You must assign the results of the PREPARE statement to a variable since it will be required when you EXECUTE the statement. The SQL statement must have any variables replaced with a question mark ? . For instance, if you wanted to insert a single value into a table you would use the following syntax: stmt = %sql PREPARE insert into x values (?) One important note with parameter markers. If you require the parameter to have a specific data type (say INTEGER) then you may want to place a CAST statement around it to force the proper conversion. Usually strings, integers, decimals, etc., convert fine when using this syntax, but occasionally you may run across a data type issue. For the previous example we could modify it to: stmt = %sql PREPARE insert into x values (CAST(? AS INTEGER)) Once you have prepared a statement, you can execute it using the following syntax: %sql EXECUTE :stmt USING :v1,:v2,:v3,.... You must provide the statement variable :stmt to the EXECUTE statement so that it knows which prepared code to execute. You can create many prepared statements and use them throughout your code. The values that following the USING clause are either constants or SQL host variable names separated by commas. If you place a colon : in front of a variable name, it will be immediately substituted into the statement: %sql EXECUTE :stmt USING 3,'asdsa',24.5,:x,x Variables without a colon in front of them are linked in dynamically. When the EXECUTE statement is processed, the value in the variable is taken directly from memory so that there is no conflict with data type, quotes, or anything that might be interpreted incorrectly. In the sample above we have the same variable specified twice: :x,x Assuming that x has a value of \"Hello\", when the statement is compiled, it will look like: EXECUTE 0x7f2ef37782d0 USING 3,'asdsa',24.5,'Hello',x The contents of :stmt and :x are placed into the string and then the statement is executed. The variable x is left as is. When execution of the prepared statement occurs, the value of the variable x is linked directly to the SQL statement so that the data is retrieved directly from the variable rather than it being materialized in the SQL statement. You can use either format when creating your statements. There is a performance penalty when using the :var format since the value must be materialized into the SQL statement, and then parsed again. When using linked variables you can specify what the underlying data type is so that Db2 does not try to incorrectly translate a value. The previous section mentioned the use of the CAST function to ensure the proper data type is used. With linked variables, you can specify four types of data: char - character data type (default) bin , binary - binary data dec , decimal - decimal data type int , integer - numeric data type These modifiers are added after the variable name by using the @ symbol: %sql EXECUTE :stmt USING v1@int, v2@binary, v3@decimal The default is to treat variables as character strings.","title":"PREPARE and EXECUTE"},{"location":"developmentsql/#using-arrays-and-multiple-parameters","text":"When using the PREPARE statement, it can become cumbersome when dealing with many parameter markers. For instance, in order to insert 10 columns into a table the code would look similar to this: stmt = %sql PREPARE INSERT INTO X VALUES (?,?,?,?,?,?,?,?,?,?) The %sql command allows you to use the short form ?*# where # is an integer representing the number of columns you want in the list. The above statement could be written as: stmt = %sql PREPARE INSERT INTO X VALUES (?*10) The syntax can also be used to create groups of parameter markers: stmt = %sql PREPARE INSERT INTO X VALUES (?*3,?*7) While this may seem a strange way of providing parameters, this becomes more useful when we use the EXECUTE command. The EXECUTE command can use Python arrays (lists) as input arguments. For the previous example with 10 parameters you could issue the following command: %sql EXECUTE :stmt USING :v1,:v2,:v3,:v4,:v5,:v6,:v7,:v8,:v9,:v10 If you placed all of these values into an array, you could also do the following: %sql EXECUTE :stmt USING :v[0],:v[1],:v[2],:v[3],:v[4],:v[5],:v[6],:v[7],:v[8],:v[9] That isn't much simpler but shows that you could use items within an array (one dimensional only). The easiest syntax is: %sql EXECUTE :stmt USING :v The only requirement is that the array v has exactly the number of values required to satisfy the parameter list required by the prepared statement. When you split the argument list into groups, you can use multiple arrays to contain the data. Given the following prepare statement: stmt = %sql PREPARE INSERT INTO X VALUES (?*3,?*7) You could execute the statement using two arrays: %sql EXECUTE :stmt USING :name, :details This would work as long as the total number of parameters supplied by the name array and details array is equal to 10.","title":"Using Arrays and Multiple Parameters"},{"location":"developmentsql/#performance-comparisons","text":"The following examples will show the use of AUTOCOMMIT and PREPARE / EXECUTE when running SQL statements. This first SQL statement will load the EMPLOYEE and DEPARTMENT tables (if they don't already exist) and then return an array of the employees in the company using a SELECT statement. The data is returned as a Python list (array) with the -r option. %sql SAMPLEDATA employees = %sql -r select * from employee The employees variable contains the employee data as a Python array. The next statement will retrieve the contents of the first row only (Remember that row 0 contains the name of the columns). We now will create another EMPLOYEE table that is an exact duplicate of what we already have. %%sql -q DROP TABLE EMPLOYEE2; CREATE TABLE EMPLOYEE2 AS (SELECT * FROM EMPLOYEE) DEFINITION ONLY;","title":"Performance Comparisons"},{"location":"developmentsql/#loop-with-insert-statements","text":"One could always use SQL to insert into this table, but we will use a loop to execute insert statements. The loop will be timed so that we can get a sense of the cost of running this code. In order to make the loop run bit a longer the insert block is run 50 times.","title":"Loop with INSERT Statements"},{"location":"developmentsql/#loop-with-prepare-statement","text":"An alternative method would be to use a prepared statement that allows us to compile the statement once in Db2 and then reuse the statement in Db2's memory. This method uses the individual column values as input into the EXECUTE statement. Note that this code is run with AUTOCOMMIT OFF which tells Db2 to commit every INSERT statement. Using AUTOCOMMIT OFF results in a lot of additional overhead when running this code.","title":"Loop with PREPARE Statement"},{"location":"developmentsql/#loop-with-prepare-statement-and-arrays","text":"You will notice that it is a bit tedious to write out the columns that are required as part of an INSERT statement. A simpler option is to use a Python list or array to and assign it directly in the EXECUTE statement. So rather than: %sql execute :prep using :empno, :firstnme, ... We can just use the array variable generated as part of the for loop: %sql execute :prep using :record The following SQL demonstrates this approach.","title":"Loop with PREPARE Statement and Arrays"},{"location":"developmentsql/#loop-with-prepare-statement-arrays-and-autocommit-off","text":"Finally, we can turn AUTOCOMMIT off and then commit the work at the end of the block to improve the total time required to insert the data. Note the use of the parameter short form ?*14 in the code.","title":"Loop with PREPARE Statement, Arrays and AUTOCOMMIT OFF"},{"location":"developmentsql/#performance-comparison","text":"You may have noticed that the performance of the last method is substantially faster than the other examples. The primary reason for this is the COMMIT only occurring at the end of the code. Run Elapsed Time Insert 3.55 Prepare 2.21 Array 1.47 Commit .67","title":"Performance Comparison"},{"location":"developmentsql/#error-handling","text":"When using PREPARE and EXECUTE statements, the feedback from SQL statements is done via four pre-defined variables: sqlcode - The SQLCODE returned by the last statement executed sqlstate - The SQLSTATE returned by the last statement executed sqlelapsed - The time taken to execute the last statement sqlerror - The error message associated with the sqlcode value When issuing an SQL statement via the %sql command, the feedback is immediate on an error: However, the PREPARE statement does not provide any messages or feedback: This is due to the way PREPARE works. It checks the validity of the SQL, but it doesn't generate an error message because there is a possibility that the object EMPLOYEE4 may exist when the statement gets executed. When you do execute the prepared statement, an error message will be produced. For this reason, the sqlcode should be checked immediately after any EXECUTE statement to make sure that the SQL executed successfully. Usually an sqlcode that is negative is an error that needs to be reviewed. Positive sqlcode value may need to be reviewed, but the following values can usually be ignored: 0 - Statement executed successfully 100 - No records were found Another variable called sqlelapsed may be useful in determining the performance of your SQL statements. The sqlelapsed variable contains the execution time of the SQL, not including the time taken to retrieve the answer set.","title":"Error Handling"},{"location":"extramacros/","text":"Predefined SQL Macros There are four %sql commands that are implemented as macros. These commands are: SAMPLEDATA LIST TABLES DESCRIBE SET <option> <value> These commands would be used in the same way that SQL statement are executed, either in a %sql or %%sql block. Sample Data Many of the examples that are used when learning Db2 revolve around the use of the EMPLOYEE and DEPARTMENT tables. Normally these tables would be created as part of the SAMPLE database. If you have direct access to the Db2 database, you can create the SAMPLE database with the following command. Note that you must be connected to the Db2 Instance and have a terminal (command line) window open. db2sampl -sql -xml There are additional options available for the db2sampl command in the Db2 documentation. If you are using another database, or do not have access to the db2sampl command, this macro will generate the EMPLOYEE and DEPARTMENT tables. The SAMPLE database contains many more tables, but these two form the basis for many SQL examples. To create these tables under your currently connected userid, issue the following command: %sql SAMPLEDATA This command will generate the two demonstration tables under the current schema. Note that if the EMPLOYEE or DEPARTMENT table are found, they will not be replaced or refreshed. This is to avoid over-writing tables that may have been generated with the db2sampl command. List Tables The LIST TABLES command is useful when you are searching for the tables within in your database. The syntax for this command is: %sql LIST TABLES [FOR SCHEMA schema] Issuing the LIST TABLES without any arguments will produce a list of tables for the current schema. If the FOR SCHEMA schema option is supplied, all tables for that schema will be displayed. Describe a Table The DESCRIBE command will display the name, data type, length, scale, and nullability for all columns in a table. The syntax for the DESCRIBE command is: %sql DESCRIBE [TABLE table] | [SELECT statement] The simplest form of the command lists the columns of a table as shown in the example below. The command is also useful when trying to determine the data types of an SQL statement. The following example illustrates how a calculation in a select list is returned in the answer set. There is more detailed information displayed when you describe a SELECT statement. Set OPTIONS The SET command can be used to set any of the Db2 magic options: SET MAXROWS value SET MAXGRID value SET THREADS value SET DISPLAY value These commands can also be set using the OPTION keyword: OPTION MAXROWS value You can use either form of the command. The SET command is what has been typically used for changing Db2 settings, so this aligns that practice.","title":"Predefined Macros"},{"location":"extramacros/#predefined-sql-macros","text":"There are four %sql commands that are implemented as macros. These commands are: SAMPLEDATA LIST TABLES DESCRIBE SET <option> <value> These commands would be used in the same way that SQL statement are executed, either in a %sql or %%sql block.","title":"Predefined SQL Macros"},{"location":"extramacros/#sample-data","text":"Many of the examples that are used when learning Db2 revolve around the use of the EMPLOYEE and DEPARTMENT tables. Normally these tables would be created as part of the SAMPLE database. If you have direct access to the Db2 database, you can create the SAMPLE database with the following command. Note that you must be connected to the Db2 Instance and have a terminal (command line) window open. db2sampl -sql -xml There are additional options available for the db2sampl command in the Db2 documentation. If you are using another database, or do not have access to the db2sampl command, this macro will generate the EMPLOYEE and DEPARTMENT tables. The SAMPLE database contains many more tables, but these two form the basis for many SQL examples. To create these tables under your currently connected userid, issue the following command: %sql SAMPLEDATA This command will generate the two demonstration tables under the current schema. Note that if the EMPLOYEE or DEPARTMENT table are found, they will not be replaced or refreshed. This is to avoid over-writing tables that may have been generated with the db2sampl command.","title":"Sample Data"},{"location":"extramacros/#list-tables","text":"The LIST TABLES command is useful when you are searching for the tables within in your database. The syntax for this command is: %sql LIST TABLES [FOR SCHEMA schema] Issuing the LIST TABLES without any arguments will produce a list of tables for the current schema. If the FOR SCHEMA schema option is supplied, all tables for that schema will be displayed.","title":"List Tables"},{"location":"extramacros/#describe-a-table","text":"The DESCRIBE command will display the name, data type, length, scale, and nullability for all columns in a table. The syntax for the DESCRIBE command is: %sql DESCRIBE [TABLE table] | [SELECT statement] The simplest form of the command lists the columns of a table as shown in the example below. The command is also useful when trying to determine the data types of an SQL statement. The following example illustrates how a calculation in a select list is returned in the answer set. There is more detailed information displayed when you describe a SELECT statement.","title":"Describe a Table"},{"location":"extramacros/#set-options","text":"The SET command can be used to set any of the Db2 magic options: SET MAXROWS value SET MAXGRID value SET THREADS value SET DISPLAY value These commands can also be set using the OPTION keyword: OPTION MAXROWS value You can use either form of the command. The SET command is what has been typically used for changing Db2 settings, so this aligns that practice.","title":"Set OPTIONS"},{"location":"installation/","text":"Db2 Magic Commands Installation Prerequisities In order to use Db2 magic commands in your Jupyter notebook environment, you must have following prerequisites: Jupyter notebooks with a Python 3+ interpreter IBM Python driver (ibm_db) Db2 LUW access Optional: ipydatagrid, pandas, multi-threading The Db2 magic commands have been tested on Linux, UNIX, and Windows environments, but are not certified to run against Db2 for z, or Db2 for iSeries versions. This doesn't mean that they can't be adapted to work against these data sources, only that there has been no validation testing done against those target environments. If you are using Watson Studio, or are using a Cloud Pak for Data system, the Db2 Python client will have already been installed. If not, you will need the system administrator to install this feature. The main project page for the Python client can be found here . If you have access to your Jupyter notebook environment, the Db2 Python client can be installed in one of three ways: python3 -m pip install ibm_db or pip install ibm_db easy_install ibm_db conda install ibm_db Prior to running the installation you may want to run these commands to ensure the proper libraries are available for the Db2 drivers: RHEL/CentOS yum install python3-dev Ubuntu apt-get install python3-dev More detailed instructions can be found on the Db2 Python Driver support page. Optional Components There are three libraries that the Db2 magic commands load in and will use if available: Pandas dataframes 1.3 data type conversion ipydatagrid Display Multi-processing Pandas 1.3 Support If your current version of Pandas is equal to 1.3 or greater, the Db2 magic commands are able to load data into a data frame using a more efficient storage representation. To check your Pandas version, run the following command inside a Jupyter code cell: pandas.__version__ The result will be the release, version, and modification level of Pandas that you are currently running. Normally the Pandas control will take Db2 strings and represent them as objects, and decimal numeric types and convert them to float. In the 1.3 release of Pandas, the type of data type can be explicitly set, and this will significantly reduce the amount of memory that Pandas requires to store the Db2 answer set. See read_sql_query for more details of this feature. In the event you do not have a version that supports this feature, the Db2 magic commands will print a warning message. ipydatagrid Display The Db2 magic command will check to see whether you have the ipydatagrid control installed in your Jupyter environment. If it is not available, it will print a warning message: The default method of displaying result sets is to use Pandas formatting. For instance, the following result set is produced from querying the contents of the sample EMPLOYEE table: The amount of data that is displayed is limited to 10 rows, with the first and last five rows being shown. There are ways to adjust the number of rows displayed, and the settings are covered in another chapter. If you install the ipydatagrid , the results can be displayed in a scrollable window: The ipydatagrid control allows you to scroll through the entire answer set rather than having to print the contents of Pandas dataframe. This makes it much easier for you to review the results of your queries rather than having without having to adjust the Pandas display limits. To install the ipydatagrid control, use one of the following options. For a non-conda installation use: pip install ipydatagrid jupyter nbextension enable --py --sys-prefix ipydatagrid The following is only required if you have not enabled the ipywidgets nbextensions: jupyter nbextension enable --py --sys-prefix widgetsnbextension If you are using conda as the Jupyter notebook and Python distribution, use the following command: conda config --add channels conda-forge conda install ipydatagrid Multi-processing The Db2 magic command can split a query into multiple processes (independent of the Db2 LUW version). This technique is useful when a large amount of data needs to be retrieved, or if it is more efficient to search for rows based on a range of values. The multiprocessing feature will initiate \"x\" number of processes, each running the same SQL with a specific range value. To the Db2 server this looks like \"x\" number of users requesting similar data. In effect, you are forcing Db2 to do more work on your behalf! This doesn't change the performance of Db2 itself, but may allow Db2 to optimize the answer set for each thread and improve the speed of answer set retrieval. You should always check with the database administrator to verify that using multiprocessing will not cause performance issues in the database. When the Db2 magic command initializes, it will display a message if multiprocessing is not available: To install the multiprocessing package, use the following command: pip install multiprocess Details of the feature can be found here . Loading Db2 Magic Commands Once you have ibm_db installed and any additional features, you will need to download the Db2 magic commands. The Db2 magic commands can be downloaded and placed directly to the directory that your Jupyter notebooks are stored in, or can be downloaded from within a Jupyter notebook. To load the Db2 magic commands into your notebook, use the following syntax: !wget https://raw.githubusercontent.com/IBM/db2-jupyter/master/db2.ipynb -O db2.ipynb Once you have loaded the Db2 magic commands into your notebook, you are able to query Db2 tables using standard SQL syntax: The following sections will describe how to get started with the Db2 magic commands and how to become more productive when querying Db2 databases.","title":"Installation"},{"location":"installation/#db2-magic-commands-installation","text":"","title":"Db2 Magic Commands Installation"},{"location":"installation/#prerequisities","text":"In order to use Db2 magic commands in your Jupyter notebook environment, you must have following prerequisites: Jupyter notebooks with a Python 3+ interpreter IBM Python driver (ibm_db) Db2 LUW access Optional: ipydatagrid, pandas, multi-threading The Db2 magic commands have been tested on Linux, UNIX, and Windows environments, but are not certified to run against Db2 for z, or Db2 for iSeries versions. This doesn't mean that they can't be adapted to work against these data sources, only that there has been no validation testing done against those target environments. If you are using Watson Studio, or are using a Cloud Pak for Data system, the Db2 Python client will have already been installed. If not, you will need the system administrator to install this feature. The main project page for the Python client can be found here . If you have access to your Jupyter notebook environment, the Db2 Python client can be installed in one of three ways: python3 -m pip install ibm_db or pip install ibm_db easy_install ibm_db conda install ibm_db Prior to running the installation you may want to run these commands to ensure the proper libraries are available for the Db2 drivers: RHEL/CentOS yum install python3-dev Ubuntu apt-get install python3-dev More detailed instructions can be found on the Db2 Python Driver support page.","title":"Prerequisities"},{"location":"installation/#optional-components","text":"There are three libraries that the Db2 magic commands load in and will use if available: Pandas dataframes 1.3 data type conversion ipydatagrid Display Multi-processing","title":"Optional Components"},{"location":"installation/#pandas-13-support","text":"If your current version of Pandas is equal to 1.3 or greater, the Db2 magic commands are able to load data into a data frame using a more efficient storage representation. To check your Pandas version, run the following command inside a Jupyter code cell: pandas.__version__ The result will be the release, version, and modification level of Pandas that you are currently running. Normally the Pandas control will take Db2 strings and represent them as objects, and decimal numeric types and convert them to float. In the 1.3 release of Pandas, the type of data type can be explicitly set, and this will significantly reduce the amount of memory that Pandas requires to store the Db2 answer set. See read_sql_query for more details of this feature. In the event you do not have a version that supports this feature, the Db2 magic commands will print a warning message.","title":"Pandas 1.3 Support"},{"location":"installation/#ipydatagrid-display","text":"The Db2 magic command will check to see whether you have the ipydatagrid control installed in your Jupyter environment. If it is not available, it will print a warning message: The default method of displaying result sets is to use Pandas formatting. For instance, the following result set is produced from querying the contents of the sample EMPLOYEE table: The amount of data that is displayed is limited to 10 rows, with the first and last five rows being shown. There are ways to adjust the number of rows displayed, and the settings are covered in another chapter. If you install the ipydatagrid , the results can be displayed in a scrollable window: The ipydatagrid control allows you to scroll through the entire answer set rather than having to print the contents of Pandas dataframe. This makes it much easier for you to review the results of your queries rather than having without having to adjust the Pandas display limits. To install the ipydatagrid control, use one of the following options. For a non-conda installation use: pip install ipydatagrid jupyter nbextension enable --py --sys-prefix ipydatagrid The following is only required if you have not enabled the ipywidgets nbextensions: jupyter nbextension enable --py --sys-prefix widgetsnbextension If you are using conda as the Jupyter notebook and Python distribution, use the following command: conda config --add channels conda-forge conda install ipydatagrid","title":"ipydatagrid Display"},{"location":"installation/#multi-processing","text":"The Db2 magic command can split a query into multiple processes (independent of the Db2 LUW version). This technique is useful when a large amount of data needs to be retrieved, or if it is more efficient to search for rows based on a range of values. The multiprocessing feature will initiate \"x\" number of processes, each running the same SQL with a specific range value. To the Db2 server this looks like \"x\" number of users requesting similar data. In effect, you are forcing Db2 to do more work on your behalf! This doesn't change the performance of Db2 itself, but may allow Db2 to optimize the answer set for each thread and improve the speed of answer set retrieval. You should always check with the database administrator to verify that using multiprocessing will not cause performance issues in the database. When the Db2 magic command initializes, it will display a message if multiprocessing is not available: To install the multiprocessing package, use the following command: pip install multiprocess Details of the feature can be found here .","title":"Multi-processing"},{"location":"installation/#loading-db2-magic-commands","text":"Once you have ibm_db installed and any additional features, you will need to download the Db2 magic commands. The Db2 magic commands can be downloaded and placed directly to the directory that your Jupyter notebooks are stored in, or can be downloaded from within a Jupyter notebook. To load the Db2 magic commands into your notebook, use the following syntax: !wget https://raw.githubusercontent.com/IBM/db2-jupyter/master/db2.ipynb -O db2.ipynb Once you have loaded the Db2 magic commands into your notebook, you are able to query Db2 tables using standard SQL syntax: The following sections will describe how to get started with the Db2 magic commands and how to become more productive when querying Db2 databases.","title":"Loading Db2 Magic Commands"},{"location":"linevscell/","text":"SQL Magic Commands The Db2 extension is made up of one magic command that works either at the LINE level ( %sql ) or at the CELL level ( %%sql ). Both versions of the %sql commands provide the ability to run Db2 SQL commands including: DCL - Setting permissions for database objects DDL - Creating or dropping database objects DML - Insert, update, delete, or select data from tables Administration - A subset of administration commands using the ADMIN_CMD function The syntax of the %sql statement is: %sql -options sql The syntax for %%sql is: %%sql -options sql 1 sql 2 ... The difference between the two %sql commands is that %sql only allows for a single statement to be executed, while the %%sql allows a block of SQL to be executed. Line Command %sql In a Jupyter notebook, a code cell can contain Python code and magic commands. You can use the single line version of the %sql command along with Python code in a cell: The entire SQL statement must be found after the %sql command. You can use two techniques to allow for larger SQL statements, but the major restriction is that only one SQL statement is executed. To extend the SQL over multiple lines, use the Python \\ character as the last character of a string. This will allow you to continue the statement onto the next line: Note: Make absolutely sure that there are no characters, even spaces, following the \\ character, otherwise the statement will be terminated at that point. The other option is to create a Python variable that contains the SQL string using triple quotes ( ''' , \"\"\" ). If you are using variable substitution in the string then you must remember to use the f\"\"\" qualifier at the beginning of the string to ensure the variable substitution is done. When using this technique to create the SQL statement, you do not need to use the continuation characters. This will simplify the creation of your SQL. In summary, the single line version of the %sql command can run a single SQL statement and can also coexist with Python statements in the same code cell. Cell Command %%sql If you want to run a large SQL statement, or multiple SQL statements, then you must use the %%sql command. Note that when you use the %%sql form of the command, the entire contents of the cell is considered part of the command, so you cannot mix Python statements in the cell. The following is an example of using the %%sql command: The cell contains only the SQL statement and nothing else. The previous %sql examples combined Python and the magic command. A %%sql cell block can contain more than one SQL statement: If there is more than one SQL statement in the block, you must terminate each statement with a semicolon ; . If you are creating stored procedures or functions, then you may need to change the delimiter to an at @ sign by using the option -d . %%sql -d DROP TABLE NEW_EMPLOYEE@ CREATE TABLE NEW_EMPLOYEE LIKE EMPLOYEE@ Output and Assignment Statements The %sql and %%sql commands will generate output when a SELECT statement produces results. Both of forms of the command will display the results as a Pandas dataframe. The %sql command provides the option of assigning the output to a Python variable: results = %sql SELECT * FROM DEPARTMENT The results variable will contain the answer set as a Pandas dataframe. You can use the various Pandas functions to manipulate the dataframe including the ability to display certain portions of the data, do calculations across ranges, and plot the data. The ability to assign a result set to a Python variable is only available for the %sql command. Multiple Result Sets When using the %%sql command, you could have multiple SELECT statements in your SQL: %%sql SELECT * FROM EMPLOYEE; SELECT * FROM DEPARTMENT; It's not recommended that you do this because of the way the dataframes are displayed. By default, the Db2 magic commands will display the results in the Pandas dataframe format. When you execute the above code, you will only see one result set displayed: The first SELECT is the one that is always displayed. If you do want to see multiple result sets, you must use the -grid option and have the ipydatagrid extension installed. If you use the -grid option, then the two answer sets will be displayed. Python versus Db2 Character Strings Character strings require special handling when dealing with Db2. The single quote character ' is reserved for delimiting string constants, while the double quote \" is used for naming columns that require special characters. You cannot use the double quote character to delimit strings that happen to contain the single quote character. What Db2 requires for single quotes in a string is to place two quotes in a row to have them interpreted as a single quote character. For instance, the next statement will select one employee from the table who has a quote in their last name: O'CONNELL . %sql SELECT * FROM EMPLOYEE WHERE LASTNAME = 'O''CONNELL' Python handles quotes differently! You can assign a string to a Python variable using single or double quotes. The following assignment statements are not identical! If you use the same syntax as Db2, Python will remove the quote in the string! It interprets this as two strings ( O and CONNELL ) being concatenated together. That probably isn't what you want! So the safest approach is to use double quotes around your string when you assign it to a variable. Then you can use the variable in the SQL statement as shown in the following example. Notice how the string constant was updated to contain two quotes when inserted into the SQL statement. This is done automatically by the %sql magic command, so there is no need to use the two single quotes when assigning a string to a variable. However, you must use the two single quotes when using constants in a SQL statement. Built-in Variables There are four built-in variables that you have access to in your code cells. These pre-defined variables are: sqlcode - The SQLCODE returned by the last statement executed sqlstate - The SQLSTATE returned by the last statement executed sqlelapsed - The time taken to execute the last statement sqlerror - The error message associated with the sqlcode value Normally these variables are used for application development, but you may find the sqlelapsed useful for timing SQL statements.","title":"SQL Magic Commands"},{"location":"linevscell/#sql-magic-commands","text":"The Db2 extension is made up of one magic command that works either at the LINE level ( %sql ) or at the CELL level ( %%sql ). Both versions of the %sql commands provide the ability to run Db2 SQL commands including: DCL - Setting permissions for database objects DDL - Creating or dropping database objects DML - Insert, update, delete, or select data from tables Administration - A subset of administration commands using the ADMIN_CMD function The syntax of the %sql statement is: %sql -options sql The syntax for %%sql is: %%sql -options sql 1 sql 2 ... The difference between the two %sql commands is that %sql only allows for a single statement to be executed, while the %%sql allows a block of SQL to be executed.","title":"SQL Magic Commands"},{"location":"linevscell/#line-command-sql","text":"In a Jupyter notebook, a code cell can contain Python code and magic commands. You can use the single line version of the %sql command along with Python code in a cell: The entire SQL statement must be found after the %sql command. You can use two techniques to allow for larger SQL statements, but the major restriction is that only one SQL statement is executed. To extend the SQL over multiple lines, use the Python \\ character as the last character of a string. This will allow you to continue the statement onto the next line: Note: Make absolutely sure that there are no characters, even spaces, following the \\ character, otherwise the statement will be terminated at that point. The other option is to create a Python variable that contains the SQL string using triple quotes ( ''' , \"\"\" ). If you are using variable substitution in the string then you must remember to use the f\"\"\" qualifier at the beginning of the string to ensure the variable substitution is done. When using this technique to create the SQL statement, you do not need to use the continuation characters. This will simplify the creation of your SQL. In summary, the single line version of the %sql command can run a single SQL statement and can also coexist with Python statements in the same code cell.","title":"Line Command %sql"},{"location":"linevscell/#cell-command-sql","text":"If you want to run a large SQL statement, or multiple SQL statements, then you must use the %%sql command. Note that when you use the %%sql form of the command, the entire contents of the cell is considered part of the command, so you cannot mix Python statements in the cell. The following is an example of using the %%sql command: The cell contains only the SQL statement and nothing else. The previous %sql examples combined Python and the magic command. A %%sql cell block can contain more than one SQL statement: If there is more than one SQL statement in the block, you must terminate each statement with a semicolon ; . If you are creating stored procedures or functions, then you may need to change the delimiter to an at @ sign by using the option -d . %%sql -d DROP TABLE NEW_EMPLOYEE@ CREATE TABLE NEW_EMPLOYEE LIKE EMPLOYEE@","title":"Cell Command %%sql"},{"location":"linevscell/#output-and-assignment-statements","text":"The %sql and %%sql commands will generate output when a SELECT statement produces results. Both of forms of the command will display the results as a Pandas dataframe. The %sql command provides the option of assigning the output to a Python variable: results = %sql SELECT * FROM DEPARTMENT The results variable will contain the answer set as a Pandas dataframe. You can use the various Pandas functions to manipulate the dataframe including the ability to display certain portions of the data, do calculations across ranges, and plot the data. The ability to assign a result set to a Python variable is only available for the %sql command.","title":"Output and Assignment Statements"},{"location":"linevscell/#multiple-result-sets","text":"When using the %%sql command, you could have multiple SELECT statements in your SQL: %%sql SELECT * FROM EMPLOYEE; SELECT * FROM DEPARTMENT; It's not recommended that you do this because of the way the dataframes are displayed. By default, the Db2 magic commands will display the results in the Pandas dataframe format. When you execute the above code, you will only see one result set displayed: The first SELECT is the one that is always displayed. If you do want to see multiple result sets, you must use the -grid option and have the ipydatagrid extension installed. If you use the -grid option, then the two answer sets will be displayed.","title":"Multiple Result Sets"},{"location":"linevscell/#python-versus-db2-character-strings","text":"Character strings require special handling when dealing with Db2. The single quote character ' is reserved for delimiting string constants, while the double quote \" is used for naming columns that require special characters. You cannot use the double quote character to delimit strings that happen to contain the single quote character. What Db2 requires for single quotes in a string is to place two quotes in a row to have them interpreted as a single quote character. For instance, the next statement will select one employee from the table who has a quote in their last name: O'CONNELL . %sql SELECT * FROM EMPLOYEE WHERE LASTNAME = 'O''CONNELL' Python handles quotes differently! You can assign a string to a Python variable using single or double quotes. The following assignment statements are not identical! If you use the same syntax as Db2, Python will remove the quote in the string! It interprets this as two strings ( O and CONNELL ) being concatenated together. That probably isn't what you want! So the safest approach is to use double quotes around your string when you assign it to a variable. Then you can use the variable in the SQL statement as shown in the following example. Notice how the string constant was updated to contain two quotes when inserted into the SQL statement. This is done automatically by the %sql magic command, so there is no need to use the two single quotes when assigning a string to a variable. However, you must use the two single quotes when using constants in a SQL statement.","title":"Python versus Db2 Character Strings"},{"location":"linevscell/#built-in-variables","text":"There are four built-in variables that you have access to in your code cells. These pre-defined variables are: sqlcode - The SQLCODE returned by the last statement executed sqlstate - The SQLSTATE returned by the last statement executed sqlelapsed - The time taken to execute the last statement sqlerror - The error message associated with the sqlcode value Normally these variables are used for application development, but you may find the sqlelapsed useful for timing SQL statements.","title":"Built-in Variables"},{"location":"macros/","text":"Macros The %sql command allows the use of user-defined macros. Macros can be used to generate frequently used SQL and simplify scripting. Macro substitution is done prior to any SQL being executed. Macros replace the text in the %%sql block with the SQL that is generated in the macro. For example, there are a number of commands that are available in Db2 magic that have been created as macros: LIST TABLES FOR SCHEMA DESCRIBE TABLE x SAMPLEDATA SET OPTION VALUE You can view the code in either the db2.ipynb notebook or the db2magic.py file. Macro Basics A Macro command contains logic and SQL text that you want to have substituted into your script. The %sql command will execute any macros found in text first before attempting to run the SQL. The macro name must be the first keyword after the %sql command (ignoring any flags). For instance, the following %sql command would consider LIST as the macro name: %sql LIST TABLES FOR SCHEMA DB2INST1 The %sql processor first checks to see if this first keyword has been defined as a macro. If not, it will process it as a regular SQL statement. Macros can be used in %sql lines or as part of a %%sql block. The contents of the macros are expanded and replace the text in the SQL block and then the SQL statements are executed. Macros are handled the same was as SQL statements. Anything that follows the macro name, up to the end of the SQL block, or a SQL delimiter (semicolon) are considered parameters of the macro. For instance, the LIST macro in the above example assumes that everything following the LIST keyword is a parameter. In a %%sql block, the values up to a delimiter, or end of the block, are considered part of the macro: %%sql LIST TABLES FOR SCHEMA DB2INST1; Creating a Macro To define a macro, the %%sql define <name> command is used. The body of the macro is found in the cell below the definition of the macro. This simple macro called EMPTABLE will substitute a SELECT statement into a SQL block. %%sql define emptable select empno, firstnme, lastname from employee The name of the macro follows the %%sql define command and is not case-sensitive. To use the macro, we can place it anywhere in a %sql block. This first example uses the macro by itself. The actual SQL that is generated is not shown by default. If you do want to see the SQL that gets generated, you can use the -e (echo) option to display the final SQL statement. The following example will display the generated SQL. Note that the echo setting is only used to display results for the current cell that is executing. The DEFINE command will replace any existing macro with the same name. In addition, macros only exist within your notebook and will need to be recreated if you restart the notebook kernel or re-run the %db2.ipynb notebook. If you create another Jupyter notebook, it will not contain any macros that you may have created. If there are macros that you want to share across notebooks, you should create a separate notebook and place the macro definitions in there. Then you can include these macros by executing the %run command using the name of the notebook that contains the macros. Macro Parameters and Tokens The text that follows the name of the macro is tokenized (split by spaces). In order to use any tokens that follow the macro name, you can refer to them in your macro body using the special sequence {#} where the number sign represents the nth token in the string. The tokens that are placed after the name of the macro can be any number, character string, or quoted string that is separated by blanks. Any number of parameters are permitted in a macro definition. For instance, the following string has 5 tokens: LIST 1234 \"Here's a string\" AbCD-12 'String' The tokens are LIST , 1234 , \"Here's a string\" , AbCD-12 , and 'String' . Tokens are numbered from 0 to n-1 where token number 0 (zero) is the name of the macro and 1 to n-1 (maximum of 9) are the tokens following the macro name. In the example above, token 0 is LIST while token 1 is 1234 and so on. Quotes and brackets have special handling. Quotes will ensure that spaces inside the string will not split the string into multiple tokens. Note that spaces are used as delimiters. Values found between parenthesis () will be kept together as one value regardless of the spaces that are found. Parsing is relatively simple. If you have a string similar to the following, it will not observe SQL parsing rules. LIST (\"this is\",\"parameter\") FROM EMPLOYEE The parser will generate the following tokens. LIST , ( \"this is\" , \"parameter\" ) , FROM , EMPLOYEE The values in parentheses are kept together even though there are spaces inside the brackets! From a macro perspective this means that any tokens that you want to pass to the routine needs to be kept relatively simple and separated with blanks or enclosed with parentheses. The following macro prints the employee ID, last name and salary based on an employee number list. The regular SQL would look similar to this: SELECT EMPNO, LASTNAME, SALARY FROM EMPLOYEE WHERE EMPNO IN ('000010','000020','000030') There are two ways of creating a macro to run this query. The first macro assumes that you will provide the name of the salary column as a parameter, and the employee numbers as a list in parentheses: %%sql define showemp SELECT EMPNO, LASTNAME, {1} FROM EMPLOYEE WHERE EMPNO IN {2} To execute this query your SQL would be: The other way to define the macro would be to supply the list of employee numbers as a comma-separated list and then have the macro body insert that into an IN list: %%sql define showemp SELECT EMPNO, LASTNAME, {1} FROM EMPLOYEE WHERE EMPNO IN ({*2}) To execute this query your SQL would be: Either method will work, but the second example has the advantage of working with Python arrays. Db2 magic commands will unravel a Python array into a list of values. If you provide a Python variable to the macro, the value will be substituted into the macro. If the variable happens to be a list or tuple, the items will be entered as a string of values, each separated with a comma. For instance, the following commands will print the values 1,2,3 . The showemp macro can use the same technique to select a list of employees based on the contents of a Python list or tuple. Using Token Lists As described in the previous section, tokens are numbered from 1 to n, left to right as they are found in the statement. Tokens are delimited by blanks and quotes strings will have blanks ignored. The following macro has 5 tokens: emptable lastname firstnme salary bonus '000010' Token {0} refers to the name of the macro. Here is a macro that will select data from the EMPLOYEE table using the columns listed for a specific employee number: %%sql define emptable SELECT {1},{2},{3},{4} FROM EMPLOYEE WHERE EMPNO = {5} The results are shown below. Using specific token numbers works fine if you know exactly how many tokens your SQL statement requires. In the previous example, exactly four columns are required in the answer set. What would happen if you wanted a variable number of columns to be returned in the answer set? You could add logic to the macro to provide this functionality, but it would need several if statements to cover all possibilities! The easier approach is to use a variable token list instead of a token number. The format of a variable token list is {*x} or {,x} where x refers to the token number where the list will begin. You can only have one token list in your macro and will always be at the end of your macro. When building a macro you would place the fixed tokens at the beginning of the macro definition and then the variable number of tokens will be found at the end. The {*x} will concatenate values with a blank between each value while {,x} will place a comma between each value. The previous emptable macro could be rewritten as: %%sql define emptable SELECT {*2} FROM EMPLOYEE WHERE EMPNO = {1} Macro Coding Overview Macros can contain any type of text, including SQL commands. In addition to the text, macros can also contain the following keywords: # - Comment (first character in the line) echo - Display a message return - Exit the macro but return any SQL that was generated exit - Exit the macro immediately with no generated code if/else/endif - Conditional logic var - Set a variable flags - Set flags to be used with the block The only restriction with macros is that macros cannot be nested. This means you can't call a macro from within a macro. The sections below explain the use of each of these statement types. Comment You can comment out lines or add documentation to your macros by placing a # symbol at the beginning of any line. The # symbol will only be recognized as a comment if there is nothing in front of it, otherwise it is considered part of the SQL you are generating. The following is an example of a macro definition with some comments. Exit Command and Return Command The exit command will terminate the processing within a macro and not run the generated SQL. You would use this when a condition is not met within the macro (like a missing parameter). The exit command can contain a message that will be displayed before terminating the macro. The return command will also stop the processing of the macro but will return any generated SQL back for execution. %%sql define showexit echo This message gets shown SELECT * FROM EMPLOYEE FETCH FIRST ROW ONLY exit I just exited the macro echo This message does not get shown The macro that was defined will not show the second statement, nor will it execute the SQL that was defined in the macro body. Note that the echo command displays messages in green while the exit command displays it in red. If we change the exit to a return, the SQL will be executed. Echo Command As you already noticed in the previous example, the echo command will display information on the screen. Any text following the command will have variables substituted and then displayed with a green box surrounding it. The following code illustrates the use of the command. %%sql define showecho echo Here is a message echo Two lines are shown The echo command will show each line as a separate box. If you want to have a message across multiple lines use <br> to start a new line. Var Command The var (variable) command sets a macro variable to a value. A variable is referred to in the macro script using curly braces {name}. By default, the arguments that are used in the macro call are assigned the variable names {0} to {n-1} . The variable zero contains the name of the macro, while each token after the macro name is assigned to one of these numbered variables. There is also a system variable {argc} which contains a count of the number of tokens found (including the name of the macro). To set a variable within a macro you would use the var command: var name value The variable name can be any name as long as it only includes letters, numbers, underscore _ and $. Variable names are case-sensitive so {a} and {A} are different. When the macro finishes executing, the contents of the variables will be lost. A variable can be converted to uppercase by placing the ^ beside the variable name or number. %%sql define runit echo The first parameter is {^1} The string following the variable name can include quotes and these will not be removed. %%sql define runit var hello This is a long string without quotes var hello2 'This is a long string with quotes' echo {hello} <br>{hello2} When processing a macro, each one of the tokens is automatically assigned to a variable (1, 2, ..., n) and variable zero {0} is assigned the name of the macro. The following macro will be used to show how the tokens are passed to the routine. %%sql define showvar echo Token(1)={1} <br>Token(2)={2} <br>All={*0}<br>After Token 0={*1} <br>After Token 0 with commas={,1}<br>Count={argc} Calling the macro will show how the variable names get assigned and used. If the token does not exist, a null keyword will be shown. If you use {*#} then if no values are found and an empty string is returned. Finally, any string that is supplied to the macro will include the quotes in the variable. The Hello There string will include the quotes when displayed: The count of the total number of parameters passed is found in the {argc} variable. You can use this variable to decide whether the user has supplied the proper number of tokens or change which code should be executed. %%sql define showvar echo The number of unnamed parameters is {argc}. The count of tokens does not include the name of the macro. If/Else/Endif Command If you need to add conditional logic to your macro, then you should use the if/else/endif commands. The format of the if statement is: if variable condition value statements else statements endif The else portion is optional, but the block must be closed with the endif command. If statements can be nested up to 9 levels deep: if condition 1 if condition 2 statements else if condition 3 statements end if endif endif If the condition in the if clause is true, then anything following the if statement will be executed and included in the final SQL statement. For instance, the following code will create a SQL statement based on the value of parameter 1: if {1} = \"\" SELECT * FROM EMPLOYEE else SELECT {1} FROM EMPLOYEE endif The if statement requires a condition to determine whether the block should be executed. The condition uses the following format: if {variable} condition {variable} | constant | null Variable can be a number from 1 to n which represents the tokens that are parsed during the macro call. So {1} refers to the first argument. The variable can also be the name of a named parameter. The condition is one of the following comparison operators: = , == : Equal to < : Less than > : Greater than <= , =< : Less than or equal to >= , => : Greater than or equal to != , <> : Not equal to The variable or constant will have quotes stripped away before doing the comparison. If you are testing for the existence of a variable, or to check if a token is empty, use the null keyword. %%sql define showif if {argc} = 0 exit No parameters supplied else if {argc} = \"1\" echo One parameter was supplied else echo More than one parameter was supplied: {argc} endif endif Running the previous macro with no parameters will check to see if the option keyword was used. Now include one parameter. Finally, issue the macro with multiple tokens. One additional option is available for variable substitution. If the first character of the variable name or parameter number is the ^ symbol, it will uppercase the entire string. %%sql define showif if {1} <> null echo The first token is: {^1} else echo You didn't supply a token endif The first parameter is converted to uppercase. Flags When a macro is executed, flags that are found in the %sql line or block will be used during the execution of the macro. For instance, the -e (echo) flag will be set when the following macro is executed: %sql -e showif There are circumstances where you want the macro to run with certain flags set (e.g. -q to suppress error messages). In order to set flags during the execution of a macro, use the flags command: flags -e -d ... The flags that are provided in the list will be set during the execution of the macro. One situation where you must use the flags option is for SQL blocks that require the use the alternate delimiter @ instead of a semicolon ; . Usually the @ delimiter is required for stored procedures, functions, or SQL BEGIN blocks that include multiple SQL statements that must be delimited with semicolons. In this case, the @ delimiter must be used to delimit the entire SQL block.","title":"Macros"},{"location":"macros/#macros","text":"The %sql command allows the use of user-defined macros. Macros can be used to generate frequently used SQL and simplify scripting. Macro substitution is done prior to any SQL being executed. Macros replace the text in the %%sql block with the SQL that is generated in the macro. For example, there are a number of commands that are available in Db2 magic that have been created as macros: LIST TABLES FOR SCHEMA DESCRIBE TABLE x SAMPLEDATA SET OPTION VALUE You can view the code in either the db2.ipynb notebook or the db2magic.py file.","title":"Macros"},{"location":"macros/#macro-basics","text":"A Macro command contains logic and SQL text that you want to have substituted into your script. The %sql command will execute any macros found in text first before attempting to run the SQL. The macro name must be the first keyword after the %sql command (ignoring any flags). For instance, the following %sql command would consider LIST as the macro name: %sql LIST TABLES FOR SCHEMA DB2INST1 The %sql processor first checks to see if this first keyword has been defined as a macro. If not, it will process it as a regular SQL statement. Macros can be used in %sql lines or as part of a %%sql block. The contents of the macros are expanded and replace the text in the SQL block and then the SQL statements are executed. Macros are handled the same was as SQL statements. Anything that follows the macro name, up to the end of the SQL block, or a SQL delimiter (semicolon) are considered parameters of the macro. For instance, the LIST macro in the above example assumes that everything following the LIST keyword is a parameter. In a %%sql block, the values up to a delimiter, or end of the block, are considered part of the macro: %%sql LIST TABLES FOR SCHEMA DB2INST1;","title":"Macro Basics"},{"location":"macros/#creating-a-macro","text":"To define a macro, the %%sql define <name> command is used. The body of the macro is found in the cell below the definition of the macro. This simple macro called EMPTABLE will substitute a SELECT statement into a SQL block. %%sql define emptable select empno, firstnme, lastname from employee The name of the macro follows the %%sql define command and is not case-sensitive. To use the macro, we can place it anywhere in a %sql block. This first example uses the macro by itself. The actual SQL that is generated is not shown by default. If you do want to see the SQL that gets generated, you can use the -e (echo) option to display the final SQL statement. The following example will display the generated SQL. Note that the echo setting is only used to display results for the current cell that is executing. The DEFINE command will replace any existing macro with the same name. In addition, macros only exist within your notebook and will need to be recreated if you restart the notebook kernel or re-run the %db2.ipynb notebook. If you create another Jupyter notebook, it will not contain any macros that you may have created. If there are macros that you want to share across notebooks, you should create a separate notebook and place the macro definitions in there. Then you can include these macros by executing the %run command using the name of the notebook that contains the macros.","title":"Creating a Macro"},{"location":"macros/#macro-parameters-and-tokens","text":"The text that follows the name of the macro is tokenized (split by spaces). In order to use any tokens that follow the macro name, you can refer to them in your macro body using the special sequence {#} where the number sign represents the nth token in the string. The tokens that are placed after the name of the macro can be any number, character string, or quoted string that is separated by blanks. Any number of parameters are permitted in a macro definition. For instance, the following string has 5 tokens: LIST 1234 \"Here's a string\" AbCD-12 'String' The tokens are LIST , 1234 , \"Here's a string\" , AbCD-12 , and 'String' . Tokens are numbered from 0 to n-1 where token number 0 (zero) is the name of the macro and 1 to n-1 (maximum of 9) are the tokens following the macro name. In the example above, token 0 is LIST while token 1 is 1234 and so on. Quotes and brackets have special handling. Quotes will ensure that spaces inside the string will not split the string into multiple tokens. Note that spaces are used as delimiters. Values found between parenthesis () will be kept together as one value regardless of the spaces that are found. Parsing is relatively simple. If you have a string similar to the following, it will not observe SQL parsing rules. LIST (\"this is\",\"parameter\") FROM EMPLOYEE The parser will generate the following tokens. LIST , ( \"this is\" , \"parameter\" ) , FROM , EMPLOYEE The values in parentheses are kept together even though there are spaces inside the brackets! From a macro perspective this means that any tokens that you want to pass to the routine needs to be kept relatively simple and separated with blanks or enclosed with parentheses. The following macro prints the employee ID, last name and salary based on an employee number list. The regular SQL would look similar to this: SELECT EMPNO, LASTNAME, SALARY FROM EMPLOYEE WHERE EMPNO IN ('000010','000020','000030') There are two ways of creating a macro to run this query. The first macro assumes that you will provide the name of the salary column as a parameter, and the employee numbers as a list in parentheses: %%sql define showemp SELECT EMPNO, LASTNAME, {1} FROM EMPLOYEE WHERE EMPNO IN {2} To execute this query your SQL would be: The other way to define the macro would be to supply the list of employee numbers as a comma-separated list and then have the macro body insert that into an IN list: %%sql define showemp SELECT EMPNO, LASTNAME, {1} FROM EMPLOYEE WHERE EMPNO IN ({*2}) To execute this query your SQL would be: Either method will work, but the second example has the advantage of working with Python arrays. Db2 magic commands will unravel a Python array into a list of values. If you provide a Python variable to the macro, the value will be substituted into the macro. If the variable happens to be a list or tuple, the items will be entered as a string of values, each separated with a comma. For instance, the following commands will print the values 1,2,3 . The showemp macro can use the same technique to select a list of employees based on the contents of a Python list or tuple.","title":"Macro Parameters and Tokens"},{"location":"macros/#using-token-lists","text":"As described in the previous section, tokens are numbered from 1 to n, left to right as they are found in the statement. Tokens are delimited by blanks and quotes strings will have blanks ignored. The following macro has 5 tokens: emptable lastname firstnme salary bonus '000010' Token {0} refers to the name of the macro. Here is a macro that will select data from the EMPLOYEE table using the columns listed for a specific employee number: %%sql define emptable SELECT {1},{2},{3},{4} FROM EMPLOYEE WHERE EMPNO = {5} The results are shown below. Using specific token numbers works fine if you know exactly how many tokens your SQL statement requires. In the previous example, exactly four columns are required in the answer set. What would happen if you wanted a variable number of columns to be returned in the answer set? You could add logic to the macro to provide this functionality, but it would need several if statements to cover all possibilities! The easier approach is to use a variable token list instead of a token number. The format of a variable token list is {*x} or {,x} where x refers to the token number where the list will begin. You can only have one token list in your macro and will always be at the end of your macro. When building a macro you would place the fixed tokens at the beginning of the macro definition and then the variable number of tokens will be found at the end. The {*x} will concatenate values with a blank between each value while {,x} will place a comma between each value. The previous emptable macro could be rewritten as: %%sql define emptable SELECT {*2} FROM EMPLOYEE WHERE EMPNO = {1}","title":"Using Token Lists"},{"location":"macros/#macro-coding-overview","text":"Macros can contain any type of text, including SQL commands. In addition to the text, macros can also contain the following keywords: # - Comment (first character in the line) echo - Display a message return - Exit the macro but return any SQL that was generated exit - Exit the macro immediately with no generated code if/else/endif - Conditional logic var - Set a variable flags - Set flags to be used with the block The only restriction with macros is that macros cannot be nested. This means you can't call a macro from within a macro. The sections below explain the use of each of these statement types.","title":"Macro Coding Overview"},{"location":"macros/#comment","text":"You can comment out lines or add documentation to your macros by placing a # symbol at the beginning of any line. The # symbol will only be recognized as a comment if there is nothing in front of it, otherwise it is considered part of the SQL you are generating. The following is an example of a macro definition with some comments.","title":"Comment"},{"location":"macros/#exit-command-and-return-command","text":"The exit command will terminate the processing within a macro and not run the generated SQL. You would use this when a condition is not met within the macro (like a missing parameter). The exit command can contain a message that will be displayed before terminating the macro. The return command will also stop the processing of the macro but will return any generated SQL back for execution. %%sql define showexit echo This message gets shown SELECT * FROM EMPLOYEE FETCH FIRST ROW ONLY exit I just exited the macro echo This message does not get shown The macro that was defined will not show the second statement, nor will it execute the SQL that was defined in the macro body. Note that the echo command displays messages in green while the exit command displays it in red. If we change the exit to a return, the SQL will be executed.","title":"Exit Command and Return Command"},{"location":"macros/#echo-command","text":"As you already noticed in the previous example, the echo command will display information on the screen. Any text following the command will have variables substituted and then displayed with a green box surrounding it. The following code illustrates the use of the command. %%sql define showecho echo Here is a message echo Two lines are shown The echo command will show each line as a separate box. If you want to have a message across multiple lines use <br> to start a new line.","title":"Echo Command"},{"location":"macros/#var-command","text":"The var (variable) command sets a macro variable to a value. A variable is referred to in the macro script using curly braces {name}. By default, the arguments that are used in the macro call are assigned the variable names {0} to {n-1} . The variable zero contains the name of the macro, while each token after the macro name is assigned to one of these numbered variables. There is also a system variable {argc} which contains a count of the number of tokens found (including the name of the macro). To set a variable within a macro you would use the var command: var name value The variable name can be any name as long as it only includes letters, numbers, underscore _ and $. Variable names are case-sensitive so {a} and {A} are different. When the macro finishes executing, the contents of the variables will be lost. A variable can be converted to uppercase by placing the ^ beside the variable name or number. %%sql define runit echo The first parameter is {^1} The string following the variable name can include quotes and these will not be removed. %%sql define runit var hello This is a long string without quotes var hello2 'This is a long string with quotes' echo {hello} <br>{hello2} When processing a macro, each one of the tokens is automatically assigned to a variable (1, 2, ..., n) and variable zero {0} is assigned the name of the macro. The following macro will be used to show how the tokens are passed to the routine. %%sql define showvar echo Token(1)={1} <br>Token(2)={2} <br>All={*0}<br>After Token 0={*1} <br>After Token 0 with commas={,1}<br>Count={argc} Calling the macro will show how the variable names get assigned and used. If the token does not exist, a null keyword will be shown. If you use {*#} then if no values are found and an empty string is returned. Finally, any string that is supplied to the macro will include the quotes in the variable. The Hello There string will include the quotes when displayed: The count of the total number of parameters passed is found in the {argc} variable. You can use this variable to decide whether the user has supplied the proper number of tokens or change which code should be executed. %%sql define showvar echo The number of unnamed parameters is {argc}. The count of tokens does not include the name of the macro.","title":"Var Command"},{"location":"macros/#ifelseendif-command","text":"If you need to add conditional logic to your macro, then you should use the if/else/endif commands. The format of the if statement is: if variable condition value statements else statements endif The else portion is optional, but the block must be closed with the endif command. If statements can be nested up to 9 levels deep: if condition 1 if condition 2 statements else if condition 3 statements end if endif endif If the condition in the if clause is true, then anything following the if statement will be executed and included in the final SQL statement. For instance, the following code will create a SQL statement based on the value of parameter 1: if {1} = \"\" SELECT * FROM EMPLOYEE else SELECT {1} FROM EMPLOYEE endif The if statement requires a condition to determine whether the block should be executed. The condition uses the following format: if {variable} condition {variable} | constant | null Variable can be a number from 1 to n which represents the tokens that are parsed during the macro call. So {1} refers to the first argument. The variable can also be the name of a named parameter. The condition is one of the following comparison operators: = , == : Equal to < : Less than > : Greater than <= , =< : Less than or equal to >= , => : Greater than or equal to != , <> : Not equal to The variable or constant will have quotes stripped away before doing the comparison. If you are testing for the existence of a variable, or to check if a token is empty, use the null keyword. %%sql define showif if {argc} = 0 exit No parameters supplied else if {argc} = \"1\" echo One parameter was supplied else echo More than one parameter was supplied: {argc} endif endif Running the previous macro with no parameters will check to see if the option keyword was used. Now include one parameter. Finally, issue the macro with multiple tokens. One additional option is available for variable substitution. If the first character of the variable name or parameter number is the ^ symbol, it will uppercase the entire string. %%sql define showif if {1} <> null echo The first token is: {^1} else echo You didn't supply a token endif The first parameter is converted to uppercase.","title":"If/Else/Endif Command"},{"location":"macros/#flags","text":"When a macro is executed, flags that are found in the %sql line or block will be used during the execution of the macro. For instance, the -e (echo) flag will be set when the following macro is executed: %sql -e showif There are circumstances where you want the macro to run with certain flags set (e.g. -q to suppress error messages). In order to set flags during the execution of a macro, use the flags command: flags -e -d ... The flags that are provided in the list will be set during the execution of the macro. One situation where you must use the flags option is for SQL blocks that require the use the alternate delimiter @ instead of a semicolon ; . Usually the @ delimiter is required for stored procedures, functions, or SQL BEGIN blocks that include multiple SQL statements that must be delimited with semicolons. In this case, the @ delimiter must be used to delimit the entire SQL block.","title":"Flags"},{"location":"pandasdb2/","text":"Pandas to Db2 The primary purpose of the %sql command was to allow easy access to Db2 data from within a Jupyter notebook. However, there are situations where you may want to take the contents of a Pandas dataframe and move that into a Db2 table. There are multiple steps required to move the data from a dataframe to Db2 including: Determining the columns required, including the data types associated with them Creating the Table Inserting the contents of the dataframe into the table In order to simplify the process of saving dataframe information, an extension to the SQL syntax has been added within the Db2 magic commands. The USING keyword in a SQL statement is used to trigger special handling of a dataframe. The syntax of the USING command is: USING dataframe [CREATE | REPLACE | APPEND] TABLE <name> options The purpose of the USING command is to create a table based on the dataframe definition, and optionally insert the data into the table. There are additional options which determine how the table is created and whether data is loaded into the table. The dataframe value refers to the Python variable that contains the dataframe contents. Do not use the SQL variable format :dataframe or Python value substitution {dataframe} in the statement. Create, Replace, or Append a Table The USING command has three different modes of operation: CREATE - Create the table based on the dataframe contents REPLACE - Recreate the table (delete the old one) based on the dataframe contents APPEND - Use the existing table definition and update the contents After each one of these modes you must specify the name of the table. The table can be qualified with a SCHEMA name: %sql USING df CREATE TABLE db2inst1.flights ... If no options are found after the mode ( CREATE , REPLACE ), a table will be created with the dataframe column definitions: The definition of the table is displayed underneath the command. If you believe that different data types should be used for the columns, then you can copy the command into another cell and recreate the table with the proper settings. An error message will be displayed if you use the CREATE statement and the table already exists in the system. In this case you should use the REPLACE option which will recreate the table for you. The APPEND mode will fail if the table has not been created. The best practice is to CREATE or REPLACE the table first and then use APPEND . If the table does exist, APPEND will insert the data from the dataframe into the new table. Options There are five options that can be specified after the mode: WITH DATA - Create the table and insert the data from the dataframe LIMIT x - Limit the amount of data loaded to x rows COLUMNS ASIS - Keep the column names as found in the dataframe instead of Db2-friendly names KEEP FLOAT64 - Keep all float64 dataframe columns as is KEEP INT64 - Keep all int64 dataframe columns as is With Data With the CREATE and REPLACE modes, the table will be created, but no data will be inserted in the table. The APPEND mode assumes that you do want the data inserted so it does not require this option. When you specify the WITH DATA option, after creating the table the system will insert the data from the dataframe into the Db2 table. When data is being loaded into the table, a progress indicator will appear underneath the table definition giving an indication of how many rows have been inserted into the table. LIMIT x The WITH DATA option will tell the system to insert the data from the dataframe into the Db2 table. The LIMIT x option will stop the inserts after x number of rows. This may be useful if you only want to store a subset of data into the Db2 table, or if you want to make sure that the data is being properly inserted into the table. You can always test the insert process by using the following steps: COLUMNS ASIS When the system attempts to create the Db2 table, it takes the column names in the dataframe and makes them Db2 compatible. This means that special characters and blanks are removed from the name and replaced with underscores. In addition, the names are converted to uppercase so that SQL statements do not have to delimit column names with double quotes \" . The SQL standard allows column names to contain special characters, as long as they are delimited with double quotes. If a column name uses standard characters A-Z,0-9,_ , then no quotes are required around the column name. There is also no need to worry about the case of the column names since all names will be folded to uppercase. The conversion of column names can be seen in the flights.csv data set. The columns in this CSV file are named: During the conversion process, the columns are renamed: The most notable change is the conversion of blanks to underscore _ characters. If you do not want this conversion to occur, and keep the column names in their original format, use the COLUMNS ASIS option. This will cause the program to create the table with the exact column names of the dataframe without any changes. The CREATE example below illustrates how the column names are kept the same. Note that this will make creating SQL queries slightly more challenging! If you want to query the flights per annum field, you will need to make sure you place the column name in quotes and have the correct spelling: %%sql SELECT * FROM MYFLIGHTS WHERE \"Flights p.a.\" > 10 KEEP FLOAT64, INT64 Pandas will often convert data into types that are larger than what is required. A good example of this is when a column with decimal or numeric values gets the data converted to float! The sample EMPLOYEE table illustrates this problem. A DESCRIBE of the EMPLOYEE table shows that the SALARY column is decimal. When the dataframe is created from the SELECT statement, the data type for the SALARY column is float64 . When a table is created from this dataframe, the default behavior is to create a datatype that is closer to the values in the column. In this case the data is converted to DECFLOAT which is more suited to decimal numbers. If you do not want float64 or int64 values to be converted to smaller datatypes, you must use the KEEP FLOAT64 and/or KEEP INT64 to keep the original values. %sql USING employee CREATE TABLE NEW_EMPLOYEE KEEP FLOAT64 KEEP INT64","title":"Pandas to Db2"},{"location":"pandasdb2/#pandas-to-db2","text":"The primary purpose of the %sql command was to allow easy access to Db2 data from within a Jupyter notebook. However, there are situations where you may want to take the contents of a Pandas dataframe and move that into a Db2 table. There are multiple steps required to move the data from a dataframe to Db2 including: Determining the columns required, including the data types associated with them Creating the Table Inserting the contents of the dataframe into the table In order to simplify the process of saving dataframe information, an extension to the SQL syntax has been added within the Db2 magic commands. The USING keyword in a SQL statement is used to trigger special handling of a dataframe. The syntax of the USING command is: USING dataframe [CREATE | REPLACE | APPEND] TABLE <name> options The purpose of the USING command is to create a table based on the dataframe definition, and optionally insert the data into the table. There are additional options which determine how the table is created and whether data is loaded into the table. The dataframe value refers to the Python variable that contains the dataframe contents. Do not use the SQL variable format :dataframe or Python value substitution {dataframe} in the statement.","title":"Pandas to Db2"},{"location":"pandasdb2/#create-replace-or-append-a-table","text":"The USING command has three different modes of operation: CREATE - Create the table based on the dataframe contents REPLACE - Recreate the table (delete the old one) based on the dataframe contents APPEND - Use the existing table definition and update the contents After each one of these modes you must specify the name of the table. The table can be qualified with a SCHEMA name: %sql USING df CREATE TABLE db2inst1.flights ... If no options are found after the mode ( CREATE , REPLACE ), a table will be created with the dataframe column definitions: The definition of the table is displayed underneath the command. If you believe that different data types should be used for the columns, then you can copy the command into another cell and recreate the table with the proper settings. An error message will be displayed if you use the CREATE statement and the table already exists in the system. In this case you should use the REPLACE option which will recreate the table for you. The APPEND mode will fail if the table has not been created. The best practice is to CREATE or REPLACE the table first and then use APPEND . If the table does exist, APPEND will insert the data from the dataframe into the new table.","title":"Create, Replace, or Append a Table"},{"location":"pandasdb2/#options","text":"There are five options that can be specified after the mode: WITH DATA - Create the table and insert the data from the dataframe LIMIT x - Limit the amount of data loaded to x rows COLUMNS ASIS - Keep the column names as found in the dataframe instead of Db2-friendly names KEEP FLOAT64 - Keep all float64 dataframe columns as is KEEP INT64 - Keep all int64 dataframe columns as is","title":"Options"},{"location":"pandasdb2/#with-data","text":"With the CREATE and REPLACE modes, the table will be created, but no data will be inserted in the table. The APPEND mode assumes that you do want the data inserted so it does not require this option. When you specify the WITH DATA option, after creating the table the system will insert the data from the dataframe into the Db2 table. When data is being loaded into the table, a progress indicator will appear underneath the table definition giving an indication of how many rows have been inserted into the table.","title":"With Data"},{"location":"pandasdb2/#limit-x","text":"The WITH DATA option will tell the system to insert the data from the dataframe into the Db2 table. The LIMIT x option will stop the inserts after x number of rows. This may be useful if you only want to store a subset of data into the Db2 table, or if you want to make sure that the data is being properly inserted into the table. You can always test the insert process by using the following steps:","title":"LIMIT x"},{"location":"pandasdb2/#columns-asis","text":"When the system attempts to create the Db2 table, it takes the column names in the dataframe and makes them Db2 compatible. This means that special characters and blanks are removed from the name and replaced with underscores. In addition, the names are converted to uppercase so that SQL statements do not have to delimit column names with double quotes \" . The SQL standard allows column names to contain special characters, as long as they are delimited with double quotes. If a column name uses standard characters A-Z,0-9,_ , then no quotes are required around the column name. There is also no need to worry about the case of the column names since all names will be folded to uppercase. The conversion of column names can be seen in the flights.csv data set. The columns in this CSV file are named: During the conversion process, the columns are renamed: The most notable change is the conversion of blanks to underscore _ characters. If you do not want this conversion to occur, and keep the column names in their original format, use the COLUMNS ASIS option. This will cause the program to create the table with the exact column names of the dataframe without any changes. The CREATE example below illustrates how the column names are kept the same. Note that this will make creating SQL queries slightly more challenging! If you want to query the flights per annum field, you will need to make sure you place the column name in quotes and have the correct spelling: %%sql SELECT * FROM MYFLIGHTS WHERE \"Flights p.a.\" > 10","title":"COLUMNS ASIS"},{"location":"pandasdb2/#keep-float64-int64","text":"Pandas will often convert data into types that are larger than what is required. A good example of this is when a column with decimal or numeric values gets the data converted to float! The sample EMPLOYEE table illustrates this problem. A DESCRIBE of the EMPLOYEE table shows that the SALARY column is decimal. When the dataframe is created from the SELECT statement, the data type for the SALARY column is float64 . When a table is created from this dataframe, the default behavior is to create a datatype that is closer to the values in the column. In this case the data is converted to DECFLOAT which is more suited to decimal numbers. If you do not want float64 or int64 values to be converted to smaller datatypes, you must use the KEEP FLOAT64 and/or KEEP INT64 to keep the original values. %sql USING employee CREATE TABLE NEW_EMPLOYEE KEEP FLOAT64 KEEP INT64","title":"KEEP FLOAT64, INT64"},{"location":"parallel/","text":"Parallel SQL Execution The Db2 magic commands provides a feature that lets you submit an SQL query using multiple processes. The SQL that you run in %sql and %%sql blocks are traditionally run in a single-threaded fashion. Each SQL statement is submitted to Db2 for processing and the remaining SQL does not get executed until the answer set has been retrieved for the SQL statement. Retrieving small answer sets (1000's to 10,000's of rows) is usually quick. If you need to return large amounts of data (1,000,000 of rows) then the network latency and retrieval time becomes a consideration. What the Db2 magic command provides is the ability to split the SQL workload up into separate processes. Each process would run independent of the other processes. This technique would result in: Multiple threads running SQL independently of one another Multiple result sets being returned simultaneously over the network Db2 running each SQL statement independently, resulting in higher Db2 utilization The downside is that the Db2 system administrator may not like the additional workloads being run in the database! However, the SQL processing is being done on smaller chunks of data and may be able to take advantage of scans already in memory. In addition, this technique will provide a faster mechanism for returning the data to the workstation. There is some additional complexity on the part of the user. A suitable \"range\" condition needs to be provided so that the query can be parallelized. In addition, queries that involve averages, summarized information, etc... will not work since each thread is independent and the summarization will only be true for that slice. The final answer set is automatically collapsed into one dataframe, so there is no need for the user to handle the results from multiple threads. Enabling Multi-threaded SQL The Db2 magic command checks at initialization time whether multiprocessing is enabled. If it is not, you will see a message similar to the following: To install the multiprocessing package, use the following command: pip install multiprocess Details of the feature can be found here . If multiprocessing is enabled, you will need to change the THREADS value to something between 0 and 12. For most cases a value between 4 and 8 should be sufficient. Only testing in your environment will determine what the best balance is for the thread count. SET THREADS 4 This setting applies to all queries that will be run in multithreading mode. Traditional queries will not use this option, so this can be set for your notebook, and it will not impact your normal SQL statements. Issuing Parallel SQL In order to run parallel SQL, you will need to use the following syntax: %sql USING slice [SELECT | WITH] SQL statement ... WHERE slicecolumn = :slice You can only parallelize SELECT statements. The WITH is also allowed since it refers to temporary table objects that will eventually be part of a SELECT statement. Any INSERT , DELETE , UPDATE , or MERGE command will be ignored. SQL statement that requires exclusive locks ( DELETE , UPDATE , etc...) may cause one thread to block another thread causing deadlocks and performance issues. That is why this feature is limited to querying data only. The variable that is used to parallelize the query ( slice ) must also be included in the SQL statement in the WHERE clause as an equality predicate x=:slice . Slice Value The value slice is a Python list (array) variable that contains the ranges you want to use during the execution of the SQL. The range does not have to be the same as the THREADS settings you used. The Db2 magic command will open up x threads and continue to execute the SQL query until all slice values have been used. The slice value could be a month, year, department, or any column that gives a discreet number of distinct values. For instance, the following FLIGHTS table has 19 different columns: A few columns look like candidates to use for slicing the data: FF_STATUS , FLIGHT_DATE , ORIGIN , AIRLINE . Columns that are monetary or contain hundreds of values are not good candidates. However, there are ways of generating discrete slices with ranges. This is covered in the next section. If you are interested in the flights from 3 airports, you would create a slice based on the airport codes for those locations: airports = [\"DFW\",\"AUS\",\"IAH\"] Once you have determined your slice, you must add it to your SQL and the final command would be: %sql USING airports SELECT ORIGIN, COUNT(*) FROM AIRPORTS WHERE ORIGIN = :airports The Db2 magic command will take each value in the array airports and substitute it into the SQL statement (where the :airports value is) and then run it as a separate process. The results from each process will be combined to create a final Pandas dataframe to display. The final result is shown below. The good news is that both results are the same! The key point is that the slice that you use must be found in the WHERE clause in order for each answer set to be distinct. If you do want to use summary functions like SUM or AVG , then it must be restricted to the slice value, not to the entire answer set. The following example illustrates what could go wrong. Assume you want to get the average amount of money that passengers spend on meals in Texas airports. If we restrict this to the three airports DFW , IAH , and AUS , the SQL would be: If this query was rewritten to use slices, the results would be different: You could get the proper result by returning the count and sum of meals across the three airports, but then you would need to compute the average yourself. If you save the data into a dataframe, you can easily compute the average. Incorrect Slice Values The values that are used for the slices should be of the same datatype. If not, one or more of the SQL statements may fail when executed. The program will capture the error when it occurs, but the SQL may have been run multiple times and used up resources for no reason. It is recommended that the SQL be prototyped on smaller values before running the query against the entire table. The following query demonstrates what happens when one of the slices is incorrectly typed. A simple check of the values could be done by running the SQL against a subset of the data: The clause x = :slice changes to x IN (:slice) and the FETCH FIRST 100 ROWS is added to limit the number of rows returned. Generating Slices The USING clause requires that the user know what slices to use to parallelize the query. If a column has several values, it may be easier to generate the slice using the following syntax. Assume that the column that you want to slice on is called EMPNO . empno_list = %sql -r SELECT DISTINCT(EMPNO) FROM SOME_TABLE The empno_list will contain an array of values (including the column names) of EMPNO values from the table. To generate an array of possible values to use in the USING clause, the following Python code is used: empnos = [x[0] for x in empno_list[1:]] Now that the slice values are available, the final query can be created. %sql USING empnos SELECT * FROM SOME_TABLE WHERE EMPNO = :empnos Here is an example of this technique being used to query a FLIGHTS table based on the ORIGIN airports. Multi-value Slices You may have situations where there isn't an exact value that can be used to represent the slice that you want to retrieve. For example, you may have range of values you want to use to represent a section of the data. You can use an array of arrays to achieve this. Similar to the other examples, you would create an array of values that you want to use. One of the columns in our FLIGHTS table includes the departure hour. The following array creates four blocks of times that will be used to split up the query: hours = [ [0,1,2,3,4,5],[6,7,8,9,10,11],[12,13,14,15,16,17],[18,19,20,21,22,23] ] The final query is displayed below. When using a slice that contains array values, you must use COLUMN_NAME IN (:slice_name) in your SQL rather than COLUMN_NAME = :slice_name . The value you are passing is a list of values so the only way to compare multiple values is through an IN list. SQL and Slice Debugging If there is a situation where the answer set does not appear correct, or you are receiving an SQL error message, you may want to use the -e (echo) option to display the SQL that is generated in each slice. The FLIGHTS example in the previous section can be modified to display the SQL that is being used in the individual slices. If you received an error message about invalid slice values, the -e option will usually show you the offending statement. In this example, the value 'A' is not compatible with the DEPARTURE_COLUMN datatype.","title":"Parallel SQL"},{"location":"parallel/#parallel-sql-execution","text":"The Db2 magic commands provides a feature that lets you submit an SQL query using multiple processes. The SQL that you run in %sql and %%sql blocks are traditionally run in a single-threaded fashion. Each SQL statement is submitted to Db2 for processing and the remaining SQL does not get executed until the answer set has been retrieved for the SQL statement. Retrieving small answer sets (1000's to 10,000's of rows) is usually quick. If you need to return large amounts of data (1,000,000 of rows) then the network latency and retrieval time becomes a consideration. What the Db2 magic command provides is the ability to split the SQL workload up into separate processes. Each process would run independent of the other processes. This technique would result in: Multiple threads running SQL independently of one another Multiple result sets being returned simultaneously over the network Db2 running each SQL statement independently, resulting in higher Db2 utilization The downside is that the Db2 system administrator may not like the additional workloads being run in the database! However, the SQL processing is being done on smaller chunks of data and may be able to take advantage of scans already in memory. In addition, this technique will provide a faster mechanism for returning the data to the workstation. There is some additional complexity on the part of the user. A suitable \"range\" condition needs to be provided so that the query can be parallelized. In addition, queries that involve averages, summarized information, etc... will not work since each thread is independent and the summarization will only be true for that slice. The final answer set is automatically collapsed into one dataframe, so there is no need for the user to handle the results from multiple threads.","title":"Parallel SQL Execution"},{"location":"parallel/#enabling-multi-threaded-sql","text":"The Db2 magic command checks at initialization time whether multiprocessing is enabled. If it is not, you will see a message similar to the following: To install the multiprocessing package, use the following command: pip install multiprocess Details of the feature can be found here . If multiprocessing is enabled, you will need to change the THREADS value to something between 0 and 12. For most cases a value between 4 and 8 should be sufficient. Only testing in your environment will determine what the best balance is for the thread count. SET THREADS 4 This setting applies to all queries that will be run in multithreading mode. Traditional queries will not use this option, so this can be set for your notebook, and it will not impact your normal SQL statements.","title":"Enabling Multi-threaded SQL"},{"location":"parallel/#issuing-parallel-sql","text":"In order to run parallel SQL, you will need to use the following syntax: %sql USING slice [SELECT | WITH] SQL statement ... WHERE slicecolumn = :slice You can only parallelize SELECT statements. The WITH is also allowed since it refers to temporary table objects that will eventually be part of a SELECT statement. Any INSERT , DELETE , UPDATE , or MERGE command will be ignored. SQL statement that requires exclusive locks ( DELETE , UPDATE , etc...) may cause one thread to block another thread causing deadlocks and performance issues. That is why this feature is limited to querying data only. The variable that is used to parallelize the query ( slice ) must also be included in the SQL statement in the WHERE clause as an equality predicate x=:slice .","title":"Issuing Parallel SQL"},{"location":"parallel/#slice-value","text":"The value slice is a Python list (array) variable that contains the ranges you want to use during the execution of the SQL. The range does not have to be the same as the THREADS settings you used. The Db2 magic command will open up x threads and continue to execute the SQL query until all slice values have been used. The slice value could be a month, year, department, or any column that gives a discreet number of distinct values. For instance, the following FLIGHTS table has 19 different columns: A few columns look like candidates to use for slicing the data: FF_STATUS , FLIGHT_DATE , ORIGIN , AIRLINE . Columns that are monetary or contain hundreds of values are not good candidates. However, there are ways of generating discrete slices with ranges. This is covered in the next section. If you are interested in the flights from 3 airports, you would create a slice based on the airport codes for those locations: airports = [\"DFW\",\"AUS\",\"IAH\"] Once you have determined your slice, you must add it to your SQL and the final command would be: %sql USING airports SELECT ORIGIN, COUNT(*) FROM AIRPORTS WHERE ORIGIN = :airports The Db2 magic command will take each value in the array airports and substitute it into the SQL statement (where the :airports value is) and then run it as a separate process. The results from each process will be combined to create a final Pandas dataframe to display. The final result is shown below. The good news is that both results are the same! The key point is that the slice that you use must be found in the WHERE clause in order for each answer set to be distinct. If you do want to use summary functions like SUM or AVG , then it must be restricted to the slice value, not to the entire answer set. The following example illustrates what could go wrong. Assume you want to get the average amount of money that passengers spend on meals in Texas airports. If we restrict this to the three airports DFW , IAH , and AUS , the SQL would be: If this query was rewritten to use slices, the results would be different: You could get the proper result by returning the count and sum of meals across the three airports, but then you would need to compute the average yourself. If you save the data into a dataframe, you can easily compute the average.","title":"Slice Value"},{"location":"parallel/#incorrect-slice-values","text":"The values that are used for the slices should be of the same datatype. If not, one or more of the SQL statements may fail when executed. The program will capture the error when it occurs, but the SQL may have been run multiple times and used up resources for no reason. It is recommended that the SQL be prototyped on smaller values before running the query against the entire table. The following query demonstrates what happens when one of the slices is incorrectly typed. A simple check of the values could be done by running the SQL against a subset of the data: The clause x = :slice changes to x IN (:slice) and the FETCH FIRST 100 ROWS is added to limit the number of rows returned.","title":"Incorrect Slice Values"},{"location":"parallel/#generating-slices","text":"The USING clause requires that the user know what slices to use to parallelize the query. If a column has several values, it may be easier to generate the slice using the following syntax. Assume that the column that you want to slice on is called EMPNO . empno_list = %sql -r SELECT DISTINCT(EMPNO) FROM SOME_TABLE The empno_list will contain an array of values (including the column names) of EMPNO values from the table. To generate an array of possible values to use in the USING clause, the following Python code is used: empnos = [x[0] for x in empno_list[1:]] Now that the slice values are available, the final query can be created. %sql USING empnos SELECT * FROM SOME_TABLE WHERE EMPNO = :empnos Here is an example of this technique being used to query a FLIGHTS table based on the ORIGIN airports.","title":"Generating Slices"},{"location":"parallel/#multi-value-slices","text":"You may have situations where there isn't an exact value that can be used to represent the slice that you want to retrieve. For example, you may have range of values you want to use to represent a section of the data. You can use an array of arrays to achieve this. Similar to the other examples, you would create an array of values that you want to use. One of the columns in our FLIGHTS table includes the departure hour. The following array creates four blocks of times that will be used to split up the query: hours = [ [0,1,2,3,4,5],[6,7,8,9,10,11],[12,13,14,15,16,17],[18,19,20,21,22,23] ] The final query is displayed below. When using a slice that contains array values, you must use COLUMN_NAME IN (:slice_name) in your SQL rather than COLUMN_NAME = :slice_name . The value you are passing is a list of values so the only way to compare multiple values is through an IN list.","title":"Multi-value Slices"},{"location":"parallel/#sql-and-slice-debugging","text":"If there is a situation where the answer set does not appear correct, or you are receiving an SQL error message, you may want to use the -e (echo) option to display the SQL that is generated in each slice. The FLIGHTS example in the previous section can be modified to display the SQL that is being used in the individual slices. If you received an error message about invalid slice values, the -e option will usually show you the offending statement. In this example, the value 'A' is not compatible with the DEPARTURE_COLUMN datatype.","title":"SQL and Slice Debugging"},{"location":"plotting/","text":"Plotting The Db2 magic commands include a very simplistic plotting feature. You may find other packages that provide much richer plotting capabilities using data from a Pandas dataframe. These plotting features are provided for quick plotting of data as either a bar, pie, or line chart. The first one or two columns of a result set must contain the values needed to plot the information. The three possible plot options are: -pb , -bar - bar chart (x,y) -pp , -pie - pie chart (y) -pl , -line - line chart (x,y) The following examples will use the EMPLOYEE table to demonstrate various plots. Here are the top 10 salaries in the table: If a result set only returns one column, the pie, line, and bar charts will not have any labels associated with the data. The following display shows the employee data as a bar, pie, and line plot. If you retrieve two columns of values, the first column is used for the labels (X axis or pie slices), and the second column contains the data. The SQL has been modified to return the average salary for each department. A bar chart ( -bar ) uses the first column for the x-axis labels and the second column for bar values. For a pie chart ( -pie ), the first column is used to label the slices, while the data comes from the second column. For a line chart ( -line ), the first column contains the labels, and the second column is used for the values. If you require more sophisticated plotting, then you may want to investigate the Matplotlib library.","title":"Plotting"},{"location":"plotting/#plotting","text":"The Db2 magic commands include a very simplistic plotting feature. You may find other packages that provide much richer plotting capabilities using data from a Pandas dataframe. These plotting features are provided for quick plotting of data as either a bar, pie, or line chart. The first one or two columns of a result set must contain the values needed to plot the information. The three possible plot options are: -pb , -bar - bar chart (x,y) -pp , -pie - pie chart (y) -pl , -line - line chart (x,y) The following examples will use the EMPLOYEE table to demonstrate various plots. Here are the top 10 salaries in the table: If a result set only returns one column, the pie, line, and bar charts will not have any labels associated with the data. The following display shows the employee data as a bar, pie, and line plot. If you retrieve two columns of values, the first column is used for the labels (X axis or pie slices), and the second column contains the data. The SQL has been modified to return the average salary for each department. A bar chart ( -bar ) uses the first column for the x-axis labels and the second column for bar values. For a pie chart ( -pie ), the first column is used to label the slices, while the data comes from the second column. For a line chart ( -line ), the first column contains the labels, and the second column is used for the values. If you require more sophisticated plotting, then you may want to investigate the Matplotlib library.","title":"Plotting"},{"location":"resultsets/","text":"Result Sets By default, any %sql block will return the contents of a result set as a table that is displayed in the notebook. The results are displayed using a feature of Pandas dataframes. The following select statement demonstrates a simple result set. You can assign the result set directly to a variable. x = %sql SELECT * FROM EMPLOYEE FETCH FIRST 3 ROWS ONLY The variable x contains the dataframe that was produced by the %sql statement, so you access the result set by using this variable or display the contents by just referring to it in a command line. There is an additional way of capturing the data through the use of the -r flag. x = %sql -r SELECT * FROM EMPLOYEE Rather than returning a dataframe result set, this option will produce a list of rows. Each row is a list itself. The column names are found in row zero (0) and the data rows start at 1. To access the first column of the first row, you would use x[1][0] to access it. The number of rows in the result set can be determined by using the length function and subtracting one for the header row. If you want to iterate over all rows and columns, you could use the following Python syntax instead of creating a for loop that goes from 0 to 41. If you don't want the header row, modify the first line to start at the first row instead of row zero. for x in rows[1:]: The string format function f\"{line}\" converts values into strings as required. If you are using the data as input into other functions, make sure to convert the values to the proper types ( str() ), otherwise you may risk getting syntax errors in your code. For example, the EDLEVEL column in the EMPLOYEE table is a number and concatenating this to a string will fail due to a conversion error.","title":"Result Sets"},{"location":"resultsets/#result-sets","text":"By default, any %sql block will return the contents of a result set as a table that is displayed in the notebook. The results are displayed using a feature of Pandas dataframes. The following select statement demonstrates a simple result set. You can assign the result set directly to a variable. x = %sql SELECT * FROM EMPLOYEE FETCH FIRST 3 ROWS ONLY The variable x contains the dataframe that was produced by the %sql statement, so you access the result set by using this variable or display the contents by just referring to it in a command line. There is an additional way of capturing the data through the use of the -r flag. x = %sql -r SELECT * FROM EMPLOYEE Rather than returning a dataframe result set, this option will produce a list of rows. Each row is a list itself. The column names are found in row zero (0) and the data rows start at 1. To access the first column of the first row, you would use x[1][0] to access it. The number of rows in the result set can be determined by using the length function and subtracting one for the header row. If you want to iterate over all rows and columns, you could use the following Python syntax instead of creating a for loop that goes from 0 to 41. If you don't want the header row, modify the first line to start at the first row instead of row zero. for x in rows[1:]: The string format function f\"{line}\" converts values into strings as required. If you are using the data as input into other functions, make sure to convert the values to the proper types ( str() ), otherwise you may risk getting syntax errors in your code. For example, the EDLEVEL column in the EMPLOYEE table is a number and concatenating this to a string will fail due to a conversion error.","title":"Result Sets"},{"location":"sql_options/","text":"SQL Options Both forms of the %sql command have options that can be used to change the behavior of the code. For both forms of the command ( %sql , %%sql ), the options must be on the same line as the command: %sql -e ... %%sql -e The only difference is that the %sql command can have SQL following the parameters, while the %%sql requires the SQL to be placed on subsequent lines. There are a number of parameters that you can specify as part of the %sql statement. -d - Use alternative statement delimiter @ -q , -quiet - Suppress messages -j - JSON formatting of the first column -json - Retrieve the result set as a JSON record -a , -all - Show all output -r , -array - Return the results into a variable (list of rows) -e , -echo - Echo macro substitution -h , -help - Display help information -line , -bar , -pie - Plot data -grid - Display results in a scrollable grid Multiple parameters are allowed on a command line. Each option should be separated by a space: %sql -a -j ... The sections below will explain the options in more detail. Delimiters -d The default delimiter for all SQL statements is the semicolon ; . However, this becomes a problem when you try to create a trigger, function, or procedure that uses SQLPL (or PL/SQL). Use the -d option to turn the SQL delimiter into the at @ sign. The semicolon is then ignored as a delimiter. For example, the following SQL will use the @ sign as the delimiter. %%sql -d DROP TABLE STUFF @ CREATE TABLE STUFF (A INT) @ INSERT INTO STUFF VALUES 1,2,3 @ SELECT * FROM STUFF @ The delimiter change will only take place for the statements following the %%sql command. Subsequent cells in the notebook will still use the semicolon. You must use the -d option for every cell that needs to use the semicolon in the script. Display all results -a , -all The default number of rows displayed for any result set is 10. You have the option of changing this option when initially connecting to the database. If you want to override the number of rows displayed, you can either update the control variable, or use the -a option. The -a option will display all rows in the answer set. For instance, the following SQL will only show 10 rows even though we have several rows in the table. You will notice that the displayed result will split the visible rows to the first 5 rows and the last 5 rows. Using the -a option will display all values. The display has been cut off, but all 41 values are displayed. If you want a scrollable list, use the -grid option which is described in the next section. You can also change the number of rows that are displayed by default by using the MAXROWS option. Grid Display -grid When output is produced in Pandas format, the size of the result set will be limited to a subset of the total number of rows. If you want to view all rows returned by the SQL, then you may want to use the -grid option. In order to use this feature, you must have the ipydatagrid library installed on your system. See the prerequisites section on details of this feature. Using the -grid option will display an answer set in a scrollable grid within the notebook: You can use the scroll bars in the display to see additional columns or rows of the answer set. The ipydatagrid display also allows for the sorting of the result sets by ascending or descending order by clicking on a column name. Note : The ipydatagrid control cannot display result sets that have duplicate columns names. For instance, the following SQL query is returned in the following Pandas format: The original SALARY column is not shown in the ipydatagrid display. The last SALARY calculation (SALARY + BONUS) is what is displayed. Attempting to use the ipydatagrid control will result in only one column being displayed. This situation can occur when you join two tables together and select columns from both. If there are columns with the same name, then the last column in the SELECT list with that name will be displayed. Quiet Mode -q Every SQL statement will result in some output. You will either get an answer set (SELECT), or an indication if the command worked. For instance, the following set of SQL will generate some error messages since the tables will probably not exist: If you know that these errors may occur, you can silence them with the -q option. SQL output will not be suppressed, so the VALUES clause will still display results. JSON Formatting of First Column -j If you are retrieving JSON data from a Db2 table, you have the option of storing it as a character string in Pandas or converting it into a dictionary in Python. Normally an SQL statement with JSON data would just be retrieved as a character string. If you use the -j option, the program will convert the row(s) into an array of dictionaries. Usually you would use the assignment statement and the %sql command when using this option. The following example illustrates multiple rows being returned and queried. Retrieve Rows as JSON Records -json The previous JSON option -j was used to convert the first column of an answer set into a Python dictionary. The -json flag will take the answer set and create one dictionary entry for the contents of the row. This function is useful when you need to pass the contents of the result set to a system that requires the data in JSON format. The following example retrieves the EMPLOYEE data for employee 000010 as a JSON record: The data is always returned as an array of JSON values with each column value in the JSON record tagged with the column name (in lower case). Retrieve Data as a List -r , -array The default storage format of a SQL result set is a Pandas dataframe. If you need to manipulate the result set as a Python list, then the -r or -array option needs to be used. The data being retrieved will be converted to a conventional Python list and can be printed or assigned to a variable. The first row (0) will contain the names of the columns being returned, while the remainder of the rows will contain the data. To refer to a row in the answer set you would use answer[x][y] where x is the row (starting at 1) and y referring to the column (starting at 0). Echo SQL -e The echo command -e will display the contents of the SQL command after all substitutions have been done. The echo command is useful when debugging your SQL when you appear to be getting incorrect results. For example, the following SQL will fail with an end-of-statement error which isn't that useful at determining what went wrong! Adding the -e option to the SQL will confirm that the variable that was used in the SQL statement did not have a proper value assigned to it. Plot Data -line , -pie , -bar The three plotting options ( -line -pie -bar ) are used to create simple plots of data. There is an entire section in the documentation that will cover the basics of using these plotting options. Display Help -h , -help , ? When the %sql command is issued with the -h option, a link to the online documentation will be produced as output. Clicking on the link will result in the help manual being displayed in another browser tab or window.","title":"SQL Options"},{"location":"sql_options/#sql-options","text":"Both forms of the %sql command have options that can be used to change the behavior of the code. For both forms of the command ( %sql , %%sql ), the options must be on the same line as the command: %sql -e ... %%sql -e The only difference is that the %sql command can have SQL following the parameters, while the %%sql requires the SQL to be placed on subsequent lines. There are a number of parameters that you can specify as part of the %sql statement. -d - Use alternative statement delimiter @ -q , -quiet - Suppress messages -j - JSON formatting of the first column -json - Retrieve the result set as a JSON record -a , -all - Show all output -r , -array - Return the results into a variable (list of rows) -e , -echo - Echo macro substitution -h , -help - Display help information -line , -bar , -pie - Plot data -grid - Display results in a scrollable grid Multiple parameters are allowed on a command line. Each option should be separated by a space: %sql -a -j ... The sections below will explain the options in more detail.","title":"SQL Options"},{"location":"sql_options/#delimiters-d","text":"The default delimiter for all SQL statements is the semicolon ; . However, this becomes a problem when you try to create a trigger, function, or procedure that uses SQLPL (or PL/SQL). Use the -d option to turn the SQL delimiter into the at @ sign. The semicolon is then ignored as a delimiter. For example, the following SQL will use the @ sign as the delimiter. %%sql -d DROP TABLE STUFF @ CREATE TABLE STUFF (A INT) @ INSERT INTO STUFF VALUES 1,2,3 @ SELECT * FROM STUFF @ The delimiter change will only take place for the statements following the %%sql command. Subsequent cells in the notebook will still use the semicolon. You must use the -d option for every cell that needs to use the semicolon in the script.","title":"Delimiters -d"},{"location":"sql_options/#display-all-results-a-all","text":"The default number of rows displayed for any result set is 10. You have the option of changing this option when initially connecting to the database. If you want to override the number of rows displayed, you can either update the control variable, or use the -a option. The -a option will display all rows in the answer set. For instance, the following SQL will only show 10 rows even though we have several rows in the table. You will notice that the displayed result will split the visible rows to the first 5 rows and the last 5 rows. Using the -a option will display all values. The display has been cut off, but all 41 values are displayed. If you want a scrollable list, use the -grid option which is described in the next section. You can also change the number of rows that are displayed by default by using the MAXROWS option.","title":"Display all results -a,-all"},{"location":"sql_options/#grid-display-grid","text":"When output is produced in Pandas format, the size of the result set will be limited to a subset of the total number of rows. If you want to view all rows returned by the SQL, then you may want to use the -grid option. In order to use this feature, you must have the ipydatagrid library installed on your system. See the prerequisites section on details of this feature. Using the -grid option will display an answer set in a scrollable grid within the notebook: You can use the scroll bars in the display to see additional columns or rows of the answer set. The ipydatagrid display also allows for the sorting of the result sets by ascending or descending order by clicking on a column name. Note : The ipydatagrid control cannot display result sets that have duplicate columns names. For instance, the following SQL query is returned in the following Pandas format: The original SALARY column is not shown in the ipydatagrid display. The last SALARY calculation (SALARY + BONUS) is what is displayed. Attempting to use the ipydatagrid control will result in only one column being displayed. This situation can occur when you join two tables together and select columns from both. If there are columns with the same name, then the last column in the SELECT list with that name will be displayed.","title":"Grid Display -grid"},{"location":"sql_options/#quiet-mode-q","text":"Every SQL statement will result in some output. You will either get an answer set (SELECT), or an indication if the command worked. For instance, the following set of SQL will generate some error messages since the tables will probably not exist: If you know that these errors may occur, you can silence them with the -q option. SQL output will not be suppressed, so the VALUES clause will still display results.","title":"Quiet Mode -q"},{"location":"sql_options/#json-formatting-of-first-column-j","text":"If you are retrieving JSON data from a Db2 table, you have the option of storing it as a character string in Pandas or converting it into a dictionary in Python. Normally an SQL statement with JSON data would just be retrieved as a character string. If you use the -j option, the program will convert the row(s) into an array of dictionaries. Usually you would use the assignment statement and the %sql command when using this option. The following example illustrates multiple rows being returned and queried.","title":"JSON Formatting of First Column -j"},{"location":"sql_options/#retrieve-rows-as-json-records-json","text":"The previous JSON option -j was used to convert the first column of an answer set into a Python dictionary. The -json flag will take the answer set and create one dictionary entry for the contents of the row. This function is useful when you need to pass the contents of the result set to a system that requires the data in JSON format. The following example retrieves the EMPLOYEE data for employee 000010 as a JSON record: The data is always returned as an array of JSON values with each column value in the JSON record tagged with the column name (in lower case).","title":"Retrieve Rows as JSON Records -json"},{"location":"sql_options/#retrieve-data-as-a-list-r-array","text":"The default storage format of a SQL result set is a Pandas dataframe. If you need to manipulate the result set as a Python list, then the -r or -array option needs to be used. The data being retrieved will be converted to a conventional Python list and can be printed or assigned to a variable. The first row (0) will contain the names of the columns being returned, while the remainder of the rows will contain the data. To refer to a row in the answer set you would use answer[x][y] where x is the row (starting at 1) and y referring to the column (starting at 0).","title":"Retrieve Data as a List -r,-array"},{"location":"sql_options/#echo-sql-e","text":"The echo command -e will display the contents of the SQL command after all substitutions have been done. The echo command is useful when debugging your SQL when you appear to be getting incorrect results. For example, the following SQL will fail with an end-of-statement error which isn't that useful at determining what went wrong! Adding the -e option to the SQL will confirm that the variable that was used in the SQL statement did not have a proper value assigned to it.","title":"Echo SQL -e"},{"location":"sql_options/#plot-data-line-pie-bar","text":"The three plotting options ( -line -pie -bar ) are used to create simple plots of data. There is an entire section in the documentation that will cover the basics of using these plotting options.","title":"Plot Data -line, -pie, -bar"},{"location":"sql_options/#display-help-h-help","text":"When the %sql command is issued with the -h option, a link to the online documentation will be produced as output. Clicking on the link will result in the help manual being displayed in another browser tab or window.","title":"Display Help -h,-help,?"},{"location":"stored_procedures/","text":"Stored Procedures The %sql command supports calls to stored procedures, but there are limitations to how it is used. The following restrictions apply: Only one answer set (cursor) is available from the stored procedure Accessing output values from the procedure requires the use of the -r (raw) flag Output is available either as a pandas dataframe, or as an array The following stored procedure returns the first and last name of all employees. There are no parameters passed to the procedure. Some important points about the stored procedure itself: The procedure must declare that there are answer sets with the DYNAMIC RESULTS SETS clause A cursor must be declared and then left open If you create more than one cursor, only the first one will get processed by the routine. %%sql -d CREATE OR REPLACE PROCEDURE SHOWEMP DYNAMIC RESULT SETS 1 begin DECLARE c1 CURSOR WITH RETURN TO CALLER FOR SELECT FIRSTNME, LASTNAME FROM EMPLOYEE; OPEN c1; end@ Calling the stored procedure is done using the standard SQL CALL procname(arguments) format. There are specific rules for supplying arguments to a stored procedure: The CALL command is supported in a %sql statement only. It cannot be used as part of a %%sql block. Null arguments must use the null keyword, rather than the Python None equivalent. Brackets () are not required for stored procedures that have no arguments. The next statement will execute the stored procedure and display the results by default. Adding the -a (all output) flag to a stored procedure call will display all results. This is the same behavior when running regular SQL statements. If you want to assign the result to a variable, you only need to add an assignment statement to the SQL. The use of the -r (raw output) flag will force the output to be converted into a two-dimensional array. The first line of the array will contain the column names, while the remainder will have the results in it. This format is useful if you want to write an application in Python to manipulate the data. %sql -r CALL SHOWEMP The stored procedure is now modified to accept a single argument which is the employee number. The employee number is supplied to the stored procedure and one record is returned with all details of the employee. %%sql -d CREATE OR REPLACE PROCEDURE SHOWEMP(in inempno char(6)) DYNAMIC RESULT SETS 1 begin DECLARE c1 CURSOR WITH RETURN TO CALLER FOR SELECT * FROM EMPLOYEE WHERE EMPNO=inempno; OPEN c1; end@ When calling this procedure we must supply the employee number as a 6 character field. The behavior will be similar to the first example with no parameters. If you use the standard %sql command without the -r parameter, only the answer set will be returned or displayed. An assignment statement with -r is slightly more complex: The first value will be the array of results Subsequent values will be the input/output parameters of the stored procedure The next SQL statement will assign the results to a single variable. results = %sql -r CALL SHOWEMP('000010') The first value in the results array ( results[0] ) is the answer set array, while the subsequent values are the parameters that are passed (or returned) by the stored procedure. results[1] will be equal to the employee number we were searching for. The use of the -r flag becomes mandatory when you are retrieving an answer set from a stored procedure, and also need to access the return results of the arguments to the stored procedure. SQL stored procedures can have input, output, and input/output values. These values are returned via the %sql command but can only be accessed when you use the -r flag. The following stored procedure will return the employees in a department and also a count of the records found. %%sql -d create or replace procedure showdept(in indeptno char(3), out rowcount int) DYNAMIC RESULT SETS 1 begin DECLARE c1 CURSOR WITH RETURN TO CALLER FOR SELECT * FROM EMPLOYEE WHERE WORKDEPT=indeptno; set rowcount = (SELECT COUNT(*) FROM EMPLOYEE WHERE WORKDEPT=indeptno); OPEN c1; end@ A normal call to this stored procedure (no flags) will return the result set. Note we must supply the second parameter here but do not supply a value since it is an output value. The null keyword must be used. To access the value of the rowcount, the -r flag must be used, and the results assigned to a variable. The first value of the results array will the answer set as an array. The second value will be the department that we requested, and the third will be the rowcount. An alternative way of accessing the results is to create an assignment statement with the answer set and all parameters supplied. The following SQL will assign the answer set and parameters directly to variables rather than an array. Stored procedures without answer sets will always return the parameters or None if nothing was supplied. This simple stored procedure increments the number that was sent to it. Stored Procedure Parameters Stored procedures can have variables passed as parameters. To pass a parameter to a stored procedure, place a colon (:) in front of the variable name: %sql CALL SHOWDEPT(:deptno,null) The python variable deptno will be substituted into the CALL statement when it is executed. %%sql -d CREATE or REPLACE PROCEDURE DEPTCOUNT(in indeptno char(3), out rowcount int) begin set rowcount = (SELECT COUNT(*) FROM EMPLOYEE WHERE WORKDEPT=indeptno); end@ The following code will go through each department number and get a count of employees by calling the stored procedure. Note that an answer set return using raw format always returns an array of rows, and each row itself is made up of an array of columns. The code needs to iterate across the rows and then across the columns. The first row of the answer set is the column names, so we skip that by using depts[1:] as the starting point. System Stored Procedures The Db2 System Stored procedures work using this syntax except for procedures that return binary XML output. At this point in time there is a limitation in retrieving this data using the Python Db2 API calls that are available. An example of a working procedure call is the REORGCHK procedure.","title":"Stored Procedures"},{"location":"stored_procedures/#stored-procedures","text":"The %sql command supports calls to stored procedures, but there are limitations to how it is used. The following restrictions apply: Only one answer set (cursor) is available from the stored procedure Accessing output values from the procedure requires the use of the -r (raw) flag Output is available either as a pandas dataframe, or as an array The following stored procedure returns the first and last name of all employees. There are no parameters passed to the procedure. Some important points about the stored procedure itself: The procedure must declare that there are answer sets with the DYNAMIC RESULTS SETS clause A cursor must be declared and then left open If you create more than one cursor, only the first one will get processed by the routine. %%sql -d CREATE OR REPLACE PROCEDURE SHOWEMP DYNAMIC RESULT SETS 1 begin DECLARE c1 CURSOR WITH RETURN TO CALLER FOR SELECT FIRSTNME, LASTNAME FROM EMPLOYEE; OPEN c1; end@ Calling the stored procedure is done using the standard SQL CALL procname(arguments) format. There are specific rules for supplying arguments to a stored procedure: The CALL command is supported in a %sql statement only. It cannot be used as part of a %%sql block. Null arguments must use the null keyword, rather than the Python None equivalent. Brackets () are not required for stored procedures that have no arguments. The next statement will execute the stored procedure and display the results by default. Adding the -a (all output) flag to a stored procedure call will display all results. This is the same behavior when running regular SQL statements. If you want to assign the result to a variable, you only need to add an assignment statement to the SQL. The use of the -r (raw output) flag will force the output to be converted into a two-dimensional array. The first line of the array will contain the column names, while the remainder will have the results in it. This format is useful if you want to write an application in Python to manipulate the data. %sql -r CALL SHOWEMP The stored procedure is now modified to accept a single argument which is the employee number. The employee number is supplied to the stored procedure and one record is returned with all details of the employee. %%sql -d CREATE OR REPLACE PROCEDURE SHOWEMP(in inempno char(6)) DYNAMIC RESULT SETS 1 begin DECLARE c1 CURSOR WITH RETURN TO CALLER FOR SELECT * FROM EMPLOYEE WHERE EMPNO=inempno; OPEN c1; end@ When calling this procedure we must supply the employee number as a 6 character field. The behavior will be similar to the first example with no parameters. If you use the standard %sql command without the -r parameter, only the answer set will be returned or displayed. An assignment statement with -r is slightly more complex: The first value will be the array of results Subsequent values will be the input/output parameters of the stored procedure The next SQL statement will assign the results to a single variable. results = %sql -r CALL SHOWEMP('000010') The first value in the results array ( results[0] ) is the answer set array, while the subsequent values are the parameters that are passed (or returned) by the stored procedure. results[1] will be equal to the employee number we were searching for. The use of the -r flag becomes mandatory when you are retrieving an answer set from a stored procedure, and also need to access the return results of the arguments to the stored procedure. SQL stored procedures can have input, output, and input/output values. These values are returned via the %sql command but can only be accessed when you use the -r flag. The following stored procedure will return the employees in a department and also a count of the records found. %%sql -d create or replace procedure showdept(in indeptno char(3), out rowcount int) DYNAMIC RESULT SETS 1 begin DECLARE c1 CURSOR WITH RETURN TO CALLER FOR SELECT * FROM EMPLOYEE WHERE WORKDEPT=indeptno; set rowcount = (SELECT COUNT(*) FROM EMPLOYEE WHERE WORKDEPT=indeptno); OPEN c1; end@ A normal call to this stored procedure (no flags) will return the result set. Note we must supply the second parameter here but do not supply a value since it is an output value. The null keyword must be used. To access the value of the rowcount, the -r flag must be used, and the results assigned to a variable. The first value of the results array will the answer set as an array. The second value will be the department that we requested, and the third will be the rowcount. An alternative way of accessing the results is to create an assignment statement with the answer set and all parameters supplied. The following SQL will assign the answer set and parameters directly to variables rather than an array. Stored procedures without answer sets will always return the parameters or None if nothing was supplied. This simple stored procedure increments the number that was sent to it.","title":"Stored Procedures"},{"location":"stored_procedures/#stored-procedure-parameters","text":"Stored procedures can have variables passed as parameters. To pass a parameter to a stored procedure, place a colon (:) in front of the variable name: %sql CALL SHOWDEPT(:deptno,null) The python variable deptno will be substituted into the CALL statement when it is executed. %%sql -d CREATE or REPLACE PROCEDURE DEPTCOUNT(in indeptno char(3), out rowcount int) begin set rowcount = (SELECT COUNT(*) FROM EMPLOYEE WHERE WORKDEPT=indeptno); end@ The following code will go through each department number and get a count of employees by calling the stored procedure. Note that an answer set return using raw format always returns an array of rows, and each row itself is made up of an array of columns. The code needs to iterate across the rows and then across the columns. The first row of the answer set is the column names, so we skip that by using depts[1:] as the starting point.","title":"Stored Procedure Parameters"},{"location":"stored_procedures/#system-stored-procedures","text":"The Db2 System Stored procedures work using this syntax except for procedures that return binary XML output. At this point in time there is a limitation in retrieving this data using the Python Db2 API calls that are available. An example of a working procedure call is the REORGCHK procedure.","title":"System Stored Procedures"},{"location":"support/","text":"Support For any questions regarding the Db2 Magic commands, including any suggestions, general comments, or bug reports, please contact: George Baklarz baklarz@ca.ibm.com Phil Downey phil.downey1@ibm.com George & Phil Acknowledgements We would like to thank the following people who helped in early prototyping, testing, suggestions, and feedback on the Db2 Magic commands. Peter Kohlmann Dean Compher","title":"Support"},{"location":"support/#support","text":"For any questions regarding the Db2 Magic commands, including any suggestions, general comments, or bug reports, please contact: George Baklarz baklarz@ca.ibm.com Phil Downey phil.downey1@ibm.com George & Phil","title":"Support"},{"location":"support/#acknowledgements","text":"We would like to thank the following people who helped in early prototyping, testing, suggestions, and feedback on the Db2 Magic commands. Peter Kohlmann Dean Compher","title":"Acknowledgements"},{"location":"variables/","text":"Variables The %sql and %%sql statements allow the use of variables within the SQL. This provides some flexibility in the writing your SQL so that you don't need to change the code when you modify one of the search values. There are two ways of passing a value to a SQL statement: Use Python notation {variable} Use SQL host variables :variable Python Variables The Python variable notation can only be used in single-line %sql statements, while the use of parameter markers is used in either style of SQL statement. The following SQL statement will illustrate the use of variables to modify the SELECT statement: %sql SELECT * FROM EMPLOYEE WHERE EMPNO = {empno} The {empno} field refers to the Python variable called empno and the contents of that variable will be substituted into the statement before execution. The code that you would place into your notebook cell would be: empno = '000010' %sql SELECT * FROM EMPLOYEE WHERE EMPNO = {empno} By echoing the command, we can see that the substitution was done before the SQL was executed: You will note that the employee number is 000010 in the SQL. Luckily, Db2 interpreted the EMPNO in the employee records as a number and compared it to the number that was supplied and was able to find a record match. When you are using Python variable substitution, you must make sure to place quotes around character strings that you need to supply to the SQL. Without quotes, it is possible that the variable will be misinterpreted. To fix this situation, place quotes around the variable as shown in this SQL. SQL Host Variables SQL Host variables are similar to Python variables, where the name of the variable is imbedded in the SQL statement. The difference with parameters markers is that the variable name is preceded with a colon : and can be found in single line %sql statements as well as in %%sql blocks. The employee number example in the previous section can be written as follows: %sql SELECT * FROM EMPLOYEE WHERE EMPNO = :empno In the case of host variables, the Db2 magic command will determine that this is a character string and will place quotes automatically around the variables. You do not need to supply quotes when using this form of parameter substitution. The host variable can also be used in a %%sql block: %%sql SELECT * FROM EMPLOYEE WHERE EMPNO = :empno Arrays and IN Lists Variables can also be array types. Arrays are expanded into multiple values, each separated by commas. This is useful when building SQL IN lists. The following example searches for 3 employees based on their employee number. empnos = ['000010','000020','000030'] %sql SELECT * FROM EMPLOYEE WHERE EMPNO IN (:empnos) Note that you must place parenthesis around the variable. SQL requires that the IN list be surrounded by parenthesis. You can reference individual array items using this technique as well. If you wanted to search for only the first value in the empnos array, use :empnos[0] instead. Dictionary Values One final type of variable substitution uses Python dictionaries. Python dictionaries resemble JSON objects and can be used to insert JSON values into Db2. For instance, the following variable contains company information in a JSON structure. customer = { \"name\" : \"Aced Hardware Stores\", \"city\" : \"Rockwood\", \"employees\" : 14 } The following SQL creates a table with a single VARCHAR column that will be used to store the JSON data. %%sql DROP TABLE SHOWJSON; CREATE TABLE SHOWJSON (INJSON VARCHAR(256)); To insert the Dictionary (JSON Record) into this Db2 table, you only need to use the variable name as one of the fields being inserted. %sql INSERT INTO SHOWJSON VALUES :customer Selecting from this table will show that the data has been inserted as a string. If you want to retrieve the data from a column that contains JSON records, you must use the -j flag to insert the contents back into a variable. The data that is returned when using the -j flag will always be an array even if the answer set is one row.","title":"Variables"},{"location":"variables/#variables","text":"The %sql and %%sql statements allow the use of variables within the SQL. This provides some flexibility in the writing your SQL so that you don't need to change the code when you modify one of the search values. There are two ways of passing a value to a SQL statement: Use Python notation {variable} Use SQL host variables :variable","title":"Variables"},{"location":"variables/#python-variables","text":"The Python variable notation can only be used in single-line %sql statements, while the use of parameter markers is used in either style of SQL statement. The following SQL statement will illustrate the use of variables to modify the SELECT statement: %sql SELECT * FROM EMPLOYEE WHERE EMPNO = {empno} The {empno} field refers to the Python variable called empno and the contents of that variable will be substituted into the statement before execution. The code that you would place into your notebook cell would be: empno = '000010' %sql SELECT * FROM EMPLOYEE WHERE EMPNO = {empno} By echoing the command, we can see that the substitution was done before the SQL was executed: You will note that the employee number is 000010 in the SQL. Luckily, Db2 interpreted the EMPNO in the employee records as a number and compared it to the number that was supplied and was able to find a record match. When you are using Python variable substitution, you must make sure to place quotes around character strings that you need to supply to the SQL. Without quotes, it is possible that the variable will be misinterpreted. To fix this situation, place quotes around the variable as shown in this SQL.","title":"Python Variables"},{"location":"variables/#sql-host-variables","text":"SQL Host variables are similar to Python variables, where the name of the variable is imbedded in the SQL statement. The difference with parameters markers is that the variable name is preceded with a colon : and can be found in single line %sql statements as well as in %%sql blocks. The employee number example in the previous section can be written as follows: %sql SELECT * FROM EMPLOYEE WHERE EMPNO = :empno In the case of host variables, the Db2 magic command will determine that this is a character string and will place quotes automatically around the variables. You do not need to supply quotes when using this form of parameter substitution. The host variable can also be used in a %%sql block: %%sql SELECT * FROM EMPLOYEE WHERE EMPNO = :empno","title":"SQL Host Variables"},{"location":"variables/#arrays-and-in-lists","text":"Variables can also be array types. Arrays are expanded into multiple values, each separated by commas. This is useful when building SQL IN lists. The following example searches for 3 employees based on their employee number. empnos = ['000010','000020','000030'] %sql SELECT * FROM EMPLOYEE WHERE EMPNO IN (:empnos) Note that you must place parenthesis around the variable. SQL requires that the IN list be surrounded by parenthesis. You can reference individual array items using this technique as well. If you wanted to search for only the first value in the empnos array, use :empnos[0] instead.","title":"Arrays and IN Lists"},{"location":"variables/#dictionary-values","text":"One final type of variable substitution uses Python dictionaries. Python dictionaries resemble JSON objects and can be used to insert JSON values into Db2. For instance, the following variable contains company information in a JSON structure. customer = { \"name\" : \"Aced Hardware Stores\", \"city\" : \"Rockwood\", \"employees\" : 14 } The following SQL creates a table with a single VARCHAR column that will be used to store the JSON data. %%sql DROP TABLE SHOWJSON; CREATE TABLE SHOWJSON (INJSON VARCHAR(256)); To insert the Dictionary (JSON Record) into this Db2 table, you only need to use the variable name as one of the fields being inserted. %sql INSERT INTO SHOWJSON VALUES :customer Selecting from this table will show that the data has been inserted as a string. If you want to retrieve the data from a column that contains JSON records, you must use the -j flag to insert the contents back into a variable. The data that is returned when using the -j flag will always be an array even if the answer set is one row.","title":"Dictionary Values"}]}